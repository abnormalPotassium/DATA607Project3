
## Analysis of Job Descriptions

Now that the data has been loaded into SQL, tidied, and explored, we will answer the project's primary question: "What are the most valued data science skills?" We will accomplish this by searching job postings on the City of New York official jobs site, identifying and quantifying requested skills in job descriptions which are part of the data science workflow. Our group will theorize about which skills in the data science workflow are "most valued" by determining how many jobs in the NYC government request those skills.

#### Connecting RMD to Database 

First, connecting to our group's database and loading helpful packages. 

```{r connect to Azure SQL Server}
library(tidyverse)
library(odbc)
library(DBI)
library(keyring)
library(scales)

if (!("Overview8909" %in% as_vector(key_list()[2]))) {
  key_set("Project3SQL","Overview8909")
}

my_connection <- dbConnect(drv = odbc::odbc(),
                           Driver = "ODBC Driver 18 for SQL Server",
                           server = "tcp:data607project3server.database.windows.net,1433",
                           database = "Data 607 Project 3 Database",
                           uid = "Overview8909",
                           pwd = key_get("Project3SQL","Overview8909"),
                           encoding = "latin1"
                           )
```


#### Pulling Data from Database

Not every field within our database is helpful for answering the project question. Here we create a custom table through a join, pulling only fields which can be searched for data science skills: business_title (e.g. data analyst), job_category (e.g. Finance), job_description, minimum_qual (e.g. college major), and preferred_skills. This table is 3,260 rows long with each row being a job posting. Making Description and p

```{r}
nyc_jobs_combination_sql <- "SELECT b.job_id, b.business_title, b.job_category, s.job_description, s.minimum_qual, s.preferred_skills FROM nyc_jobs_basics as b JOIN nyc_jobs_specifics as s ON b.job_id = s.job_id"

nyc_jobs_combination <- dbGetQuery(my_connection, nyc_jobs_combination_sql)

dbDisconnect(my_connection)

nyc_jobs_combination[,4:6] <- lapply(nyc_jobs_combination[,4:6], tolower)

nrow(nyc_jobs_combination)
```

Showing head of table
```{r}
head(nyc_jobs_combination)
```

#### Approach

For each job posting (row) we want to find all mentions of data science skills, searching them and compiling a count into a new dataframe. 

First we need to define the skills we'll search for. These keywords were generated using a combination of domain knowledge, Google Ads keyword planner, and skimming the jobs data set for requested skills. While leveraging regular expressions could make this list of keywords shorter, because there are generally so many keywords of interest, the amount of time to write regex for each would be burdensome. Therefore for this project we will searchf for whole keywords, while avoiding ambiguity.

```{r}
data_acq_keys <- tolower(
  c("SQL", "MySQL", "data acquisition", "database", "dbms", "JSON", "API", "data entry", "data scraping", "mongodb", "postgre", "mariadb", "data extraction", "web scraping", "etl", "data collection", "web crawling", "data warehouse", "azure", "Amazon Web Services" )
)
data_prep_keys <- tolower(
  c("Alteryx", "Knime",  "data engineering", "preprocessing", "data preparation", "data cleaning", "data wrangling", "data transformation", "data integration", "normalization", "imputation", "data formatting", "data merging", "data enrichment", "data augmentation", "data sampling","data reduction", "dplyr", "stringr", "pandas" )
)
data_exp_keys <- tolower(
  c("exploratory data analysis", "feature selection", "data engineering", "data exploration", "Descriptive Statistics", "Data Profiling", "Data Quality Assessment", "Data Sampling", "Dimensionality Reduction", "dimension reduction", "Feature Engineering", "Correlation Analysis", "Outlier Detection",  "databricks","data quality analysis","data summary", "data explorer", "exploratory data analysis", "kusto", "azure data explorer")
)
data_anal_keys <- tolower(
  c("data modeling", "machine learning", "data analysis", "artificial intelligence", "quantitative analysis", "predictive analysis", "regression", "Statistical Analysis", "statistical analytics", "Cluster Analysis", "Hypothesis Testing", "A/B Testing", "Data Mining", "natural language processing", "scikit-learn", "pytorch", "data analytics","pandas", "RStudio", "spss" )
)
data_vis_keys <- tolower(
  c("Tableau", "PowerBI", "quantitative findings", "creating graphs", "chart creation", "data reporting", "data visualization", "Data Storytelling", "Data Presentation", "Information Design", "Dashboard Creation", "Infographics", "network visualization", "user experience design", "datawrapper", "apache superset", "scientific visualization", "visual analytics", "visual encodings", "data animation")
)

data_science_categories <- list(
  data_acquisition = data_acq_keys, 
  data_preparation = data_prep_keys,
  data_exploration = data_exp_keys, 
  data_analysis = data_anal_keys, 
  data_visualization = data_vis_keys
)
```

#### Skill Scrape

Now we'll search each row for our combined keywords, creating a dataframe with a count of each, and a categorization.

```{r}
library(stringi)
search_keywords <- function(df, keywords, category) {
  # Create a vector to store the counts of each keyword
  word_count <- rep(0, length(keywords))
  
  # Loop through each row of the dataframe
  for (i in 1:nrow(df)) {
    # Convert all the text in each column to lowercase and combine them into a single string
    text <- paste(df[i, ], collapse = " ")
    # Loop through each keyword in the keyword list
    for (j in 1:length(keywords)) {
      # If the keyword is in the text, increment the count for that keyword
        # ignore.case to make sure either "Python" or "python" is counted
      if (stri_detect_regex(text, paste0("\\b", keywords[j], "\\b"))) {
        word_count[j] <- word_count[j] + 1
      }
    }
  }
  
  # Return the word counts as a tibble
  keywords_count <- tibble(
    category = category,
    keyword = keywords,
    count = word_count
    )
  return(keywords_count)
}
data_science_categories <- list(
  data_acquisition = data_acq_keys, 
  data_preparation = data_prep_keys,
  data_exploration = data_exp_keys, 
  data_analysis = data_anal_keys, 
  data_visualization = data_vis_keys
)
keyword_count <- rbind(
  search_keywords(nyc_jobs_combination, data_acq_keys, "Data Acquisition"),
  search_keywords(nyc_jobs_combination, data_prep_keys, "Data Preparation"),
  search_keywords(nyc_jobs_combination, data_exp_keys, "Data Exploration"),
  search_keywords(nyc_jobs_combination, data_anal_keys, "Data Analysis"),
  search_keywords(nyc_jobs_combination, data_vis_keys, "Data Visualization")
)

```

The dataframe keyword_count now shows the volume of mentions for each skill inside the NYC jobs dataset. 

```{r}
head(arrange(keyword_count,desc(count)), n = 10)
```

There were 47 individual data science skills mentioned on the NYC jobs postings.
```{r}
keyword_count |>
  filter(count>0) |>
  nrow()
```

Showing only skills with more than 10 occurrences in the dataset, then charting them. Colors based on category of Data Science skill.

To answer the primary project question, the most in-demand data science skills were "database" (459 mentions) followed by "data entry" (261), "data analysis" (214), "SQL" (193), and "data collection" (117). Four of these top five fall under the category of "Data Acquisition." 
```{r}
keyword_count |>
  filter(count>10) |>
  group_by(keyword,category) |>
  summarise(count = sum(count)) |>
  arrange(count) |>
  ggplot(aes(x= reorder(keyword,count),y=count, fill = category))+
    geom_bar(stat="identity", show.legend = TRUE) +
    xlab("Data Science Skill") +
    ylab("Job Listing Count") +
    labs(title = "Data Science Skills in NYC Job Listings", caption = "(Based on data from NYC OpenData)") +
    guides(fill=guide_legend(title="Skill Category")) +
    geom_text(aes(label=count)) +
    coord_flip()
```

Of the 3,261 jobs listed on the NYC government site, 57.5% require a data science skill.

```{r}
nyc_jobs_combination |>
  filter(total_datascience_keywords>0) |>
  nrow()/nrow(nyc_jobs_combination)
```
While 57.5% of all job posting required a data science skill, those jobs requested a varied array of skills. The three most in-demand skills, "database," "data entry," and "data analysis," were found in 14.1%, 8.0%, and 6.6% of all job postings. 

```{r}
keyword_count <- keyword_count |> 
  group_by(category,keyword) |>
  mutate(pct_all_jobs = percent(count/nrow(nyc_jobs_combination)))
```


```{r}
keyword_count |>
  filter(pct_all_jobs>=0.5) |>
  group_by(keyword,category,pct_all_jobs) |>
  summarise(pct = sum(count)/nrow(nyc_jobs_combination))|>
  ggplot(aes(x= reorder(keyword,pct),y=round(pct,2)*100, fill = category))+
    geom_bar(stat="identity", show.legend = TRUE) +
    xlab("Data Science Skill") +
    ylab("% of All Jobs Requiring Skill") +
    labs(title = "Top Data Science Skills as % of NYC Job Listings", caption = "(Based on data from NYC OpenData)") +
    guides(fill=guide_legend(title="Skill Category")) +
    geom_text(aes(label=percent(round(pct,3)))) +
    coord_flip()
```
If we combine our scraped skills list with information in the original database, we can look at the median base salaries for jobs requiring a given skill. Median base salaries are based on the median value of aggregated "Salary From" value for jobs requiring a given skill.

```{r}
ds_keywords_full <- c(data_acq_keys,data_prep_keys,data_exp_keys,data_anal_keys,data_vis_keys)

nyc_jobs_w_skills <- nyc_jobs_combination |> 
  rowwise() |>
  mutate(skill_extract = paste(ds_keywords_full[map_lgl(ds_keywords_full, ~ any(grepl(.x, c_across(business_title:preferred_skills))))], collapse = ", "))

nyc_jobs_skills <- nyc_jobs_w_skills[c('job_id','skill_extract')]
```

```{r}
library(plyr)

nyc_jobs_salaries <- dbGetQuery(my_connection, "select job_id, salary_range_from, salary_range_to, salary_frequency from nyc_jobs_basics")

salary_from_adjusted <- c()
salary_to_adjusted <- c()
freq <- ""

for(i in 1:nrow(nyc_jobs_salaries)) {
    freq <- nyc_jobs_salaries$salary_frequency[i]
    if (freq == "Annual") {
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics$salary_range_from[i], 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics$salary_range_to[i], 10000))
    } else if (freq == "Hourly") {
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics$salary_range_from[i] * 40 * 52, 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics$salary_range_to[i] * 40 * 52, 10000))
    } else { # this means it's daily
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics$salary_range_from[i] * 365, 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics$salary_range_to[i] * 365, 10000))
    }
}
nyc_jobs_salaries$salary_from_adjusted <- salary_from_adjusted
nyc_jobs_salaries$salary_to_adjusted <- salary_to_adjusted
```

```{r}
skills_salary <- merge(nyc_jobs_skills,nyc_jobs_salaries)
skills_salary <- skills_salary[c('job_id','salary_range_from','skill_extract')]

library(splitstackshape)
split_skills <- cSplit(skills_salary, "skill_extract", ", ")
```
Will do this 11x over for the 11 skill_extract_columns, forming a 2 column dataframe salary_range_median | skill where there will be duplicate skill values. Then take the group_by(skill) |> summary(median) of that 2 column dataframe to get final median value.
```{r}
df1 <- split_skills |>
  group_by(skill_extract_01) |>
  summarise(med = median(salary_range_from))
```

Skill Analysis Conclusions
- Of the 3,261 jobs listed on the NYC government site, 57.5% required a data science skill.
- There were 47 individual data science skills mentioned on the NYC jobs postings.
- The most in-demand data science skills were "database" (459 mentions) followed by "data entry" (261), "data analysis" (214), "SQL" (193), and "data collection" (117). Four of these top five fall under the data science workflow category of "Data Acquisition." 
- While 57.5% of all job posting required a data science skill, those jobs requested a varied array of skills. The three most in-demand skills, "database," "data entry," and "data analysis," were found in 14.1%, 8.0%, and 6.6% of all job postings. 
- TBD re: Salaries
