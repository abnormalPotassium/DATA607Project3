
## Analysis of Job Descriptions

Now that the data has been loaded into SQL, tidied, and explored, we will answer the project's primary question: "What are the most valued data science skills?" We will accomplish this by searching job postings on the City of New York official jobs site, identifying and quantifying requested skills in job descriptions which are part of the data science workflow. Our group will theorize about which skills in the data science workflow are "most valued" by determining how many jobs in the NYC government request those skills.

#### Connecting RMD to Database 

First, connecting to our group's database and loading helpful packages. 

```{r connect to Azure SQL Server}
library(tidyverse)
library(odbc)
library(DBI)
library(keyring)
library(scales)

if (!("Overview8909" %in% as_vector(key_list()[2]))) {
  key_set("Project3SQL","Overview8909")
}

my_connection <- dbConnect(drv = odbc::odbc(),
                           Driver = "ODBC Driver 18 for SQL Server",
                           server = "tcp:data607project3server.database.windows.net,1433",
                           database = "Data 607 Project 3 Database",
                           uid = "Overview8909",
                           pwd = key_get("Project3SQL","Overview8909"),
                           encoding = "latin1"
                           )
```


#### Pulling Data from Database

Not every field within our database is helpful for answering the project question. Here we create a custom table through a join, pulling only fields which can be searched for data science skills: business_title (e.g. data analyst), job_category (e.g. Finance), job_description, minimum_qual (e.g. college major), and preferred_skills. This table is 3,260 rows long with each row being a job posting. 

```{r}
nyc_jobs_combination_sql <- "SELECT b.job_id, b.business_title, b.job_category, s.job_description, s.minimum_qual, s.preferred_skills FROM nyc_jobs_basics as b JOIN nyc_jobs_specifics as s ON b.job_id = s.job_id"

nyc_jobs_combination <- dbGetQuery(my_connection, nyc_jobs_combination_sql)

dbDisconnect(my_connection)

nyc_jobs_combination[,4:6] <- lapply(nyc_jobs_combination[,4:6], tolower)

nrow(nyc_jobs_combination)
```

Showing head of table
```{r}
head(nyc_jobs_combination)
```

#### Approach

For each job posting (row) we want to find all mentions of data science skills, searching them and compiling a count into a new dataframe. 

First we need to define the skills we'll search for. These keywords were generated using a combination of domain knowledge, Google Ads keyword planner, and skimming the jobs data set for requested skills. While leveraging regular expressions could make this list of keywords shorter, because there are generally so many keywords of interest, the amount of time to write regex for each would be burdensome. Therefore for this project we will searchf for whole keywords, while avoiding ambiguity.

```{r}
data_acq_keys <- tolower(
  c("SQL", "MySQL", "data acquisition", "database", "dbms", "JSON", "API", "data entry", "data scraping", "mongodb", "postgre", "mariadb", "data extraction", "web scraping", "etl", "data collection", "web crawling", "data warehouse", "azure", "Amazon Web Services" )
)
data_prep_keys <- tolower(
  c("Alteryx", "Knime",  "data engineering", "preprocessing", "data preparation", "data cleaning", "data wrangling", "data transformation", "data integration", "normalization", "imputation", "data formatting", "data merging", "data enrichment", "data augmentation", "data sampling","data reduction", "dplyr", "stringr", "pandas" )
)
data_exp_keys <- tolower(
  c("exploratory data analysis", "feature selection", "data research", "data exploration", "Descriptive Statistics", "Data Profiling", "Data Quality Assessment", "Data Sampling", "Dimensionality Reduction", "dimension reduction", "Feature Engineering", "Correlation Analysis", "Outlier Detection",  "databricks","data quality analysis","data summary", "data explorer", "exploratory data analysis", "kusto", "azure data explorer")
)
data_anal_keys <- tolower(
  c("data modeling", "machine learning", "data analysis", "artificial intelligence", "quantitative analysis", "predictive analysis", "regression", "Statistical Analysis", "statistical analytics", "Cluster Analysis", "Hypothesis Testing", "A/B Testing", "Data Mining", "natural language processing", "scikit-learn", "pytorch", "data analytics","pandas", "RStudio", "spss" )
)
data_vis_keys <- tolower(
  c("Tableau", "PowerBI", "quantitative findings", "creating graphs", "chart creation", "data reporting", "data visualization", "Data Storytelling", "Data Presentation", "Information Design", "Dashboard Creation", "Infographics", "network visualization", "user experience design", "datawrapper", "apache superset", "scientific visualization", "visual analytics", "visual encodings", "data animation")
)

data_science_categories <- list(
  data_acquisition = data_acq_keys, 
  data_preparation = data_prep_keys,
  data_exploration = data_exp_keys, 
  data_analysis = data_anal_keys, 
  data_visualization = data_vis_keys
)
```

#### Skill Scrape

Now we'll search each row for our combined keywords, creating a dataframe with a count of each, and a categorization.

```{r}
library(stringi)
search_keywords <- function(df, keywords, category) {
  # Create a vector to store the counts of each keyword
  word_count <- rep(0, length(keywords))
  
  # Loop through each row of the dataframe
  for (i in 1:nrow(df)) {
    # Convert all the text in each column to lowercase and combine them into a single string
    text <- paste(df[i, ], collapse = " ")
    # Loop through each keyword in the keyword list
    for (j in 1:length(keywords)) {
      # If the keyword is in the text, increment the count for that keyword
        # ignore.case to make sure either "Python" or "python" is counted
      if (stri_detect_regex(text, paste0("\\b", keywords[j], "\\b"))) {
        word_count[j] <- word_count[j] + 1
      }
    }
  }
  
  # Return the word counts as a tibble
  keywords_count <- tibble(
    category = category,
    keyword = keywords,
    count = word_count
    )
  return(keywords_count)
}
data_science_categories <- list(
  data_acquisition = data_acq_keys, 
  data_preparation = data_prep_keys,
  data_exploration = data_exp_keys, 
  data_analysis = data_anal_keys, 
  data_visualization = data_vis_keys
)
keyword_count <- rbind(
  search_keywords(nyc_jobs_combination, data_acq_keys, "Data Acquisition"),
  search_keywords(nyc_jobs_combination, data_prep_keys, "Data Preparation"),
  search_keywords(nyc_jobs_combination, data_exp_keys, "Data Exploration"),
  search_keywords(nyc_jobs_combination, data_anal_keys, "Data Analysis"),
  search_keywords(nyc_jobs_combination, data_vis_keys, "Data Visualization")
)

```

The dataframe keyword_count now shows the volume of mentions for each skill inside the NYC jobs dataset. 

```{r}
head(arrange(keyword_count,desc(count)), n = 10)
```

There were 47 individual data science skills mentioned on the NYC jobs postings.
```{r}
keyword_count |>
  filter(count>0) |>
  nrow()
```

Showing only skills with more than 10 occurrences in the dataset, then charting them. Colors based on category of Data Science skill.

To answer the primary project question, the most in-demand data science skills were "database" (459 mentions) followed by "data entry" (261), "data analysis" (214), "SQL" (193), and "data collection" (117). Four of these top five fall under the category of "Data Acquisition." 
```{r}
keyword_count |>
  dplyr::filter(count>=10) |>
  dplyr::group_by(keyword,category) |>
  dplyr::summarise(count = sum(count)) |>
  arrange(count) |>
  ggplot(aes(x= reorder(keyword,count),y=count, fill = category))+
    geom_bar(stat="identity", show.legend = TRUE) +
    xlab("Data Science Skill") +
    ylab("Job Listing Count") +
    labs(title = "Data Science Skills in 10 or More NYC Job Listings", caption = "(Based on data from NYC OpenData)") +
    guides(fill=guide_legend(title="Skill Category")) +
    geom_text(aes(label=count)) +
    coord_flip()
```

Of the 3,261 jobs listed on the NYC government site, 57.5% require a data science skill.

```{r}
nyc_jobs_combination |>
  filter(total_datascience_keywords>0) |>
  nrow()/nrow(nyc_jobs_combination)
```
While 57.5% of all job posting required a data science skill, those jobs requested a varied array of skills. The three most in-demand skills, "database," "data entry," and "data analysis," were found in 14.1%, 8.0%, and 6.6% of all job postings. 

```{r}
keyword_count <- keyword_count |> 
  group_by(category,keyword) |>
  mutate(pct_all_jobs = percent(count/nrow(nyc_jobs_combination)))
```


```{r}
keyword_count |>
  dplyr::filter(pct_all_jobs>=0.5) |>
  dplyr::group_by(keyword,category,pct_all_jobs) |>
  dplyr::summarise(pct = sum(count)/nrow(nyc_jobs_combination))|>
  ggplot(aes(x= reorder(keyword,pct),y=round(pct,2)*100, fill = category))+
    geom_bar(stat="identity", show.legend = TRUE) +
    xlab("Data Science Skill") +
    ylab("% of All Jobs Requiring Skill") +
    labs(title = "Top Data Science Skills as % of NYC Job Listings", caption = "(Based on data from NYC OpenData)") +
    guides(fill=guide_legend(title="Skill Category")) +
    geom_text(aes(label=percent(round(pct,3)))) +
    coord_flip()
```
If we combine our scraped skills list with information in the original database, we can look at the median base salaries for jobs requiring a given skill. Median base salaries are based on the median value of aggregated "Salary From" value for jobs requiring a given skill.

```{r}
ds_keywords_full <- c(data_acq_keys,data_prep_keys,data_exp_keys,data_anal_keys,data_vis_keys)

nyc_jobs_w_skills <- nyc_jobs_combination |> 
  rowwise() |>
  dplyr::mutate(skill_extract = paste(ds_keywords_full[map_lgl(ds_keywords_full, ~ any(grepl(.x, c_across(business_title:preferred_skills))))], collapse = ", "))

nyc_jobs_skills <- nyc_jobs_w_skills[c('job_id','skill_extract')]
```

```{r}
library(plyr)

nyc_jobs_salaries <- dbGetQuery(my_connection, "select job_id, salary_range_from, salary_range_to, salary_frequency from nyc_jobs_basics")

salary_from_adjusted <- c()
salary_to_adjusted <- c()
freq <- ""

for(i in 1:nrow(nyc_jobs_salaries)) {
    freq <- nyc_jobs_salaries$salary_frequency[i]
    if (freq == "Annual") {
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics$salary_range_from[i], 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics$salary_range_to[i], 10000))
    } else if (freq == "Hourly") {
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics$salary_range_from[i] * 40 * 52, 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics$salary_range_to[i] * 40 * 52, 10000))
    } else { # this means it's daily
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics$salary_range_from[i] * 365, 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics$salary_range_to[i] * 365, 10000))
    }
}
nyc_jobs_salaries$salary_from_adjusted <- salary_from_adjusted
nyc_jobs_salaries$salary_to_adjusted <- salary_to_adjusted
```

```{r}
skills_salary <- merge(nyc_jobs_skills,nyc_jobs_salaries)
skills_salary <- skills_salary[c('salary_from_adjusted','skill_extract')]

library(splitstackshape)
split_skills <- cSplit(skills_salary, "skill_extract", ", ")

skills_long <- split_skills |>
  pivot_longer(cols=c('skill_extract_01',
                      'skill_extract_02',
                      'skill_extract_03',
                      'skill_extract_04',
                      'skill_extract_05',
                      'skill_extract_06',
                      'skill_extract_07',
                      'skill_extract_08',
                      'skill_extract_09',
                      'skill_extract_10',
                      'skill_extract_11'),
               names_to='skill_extract',
               values_to='skill')

skills_long <- skills_long[c('salary_from_adjusted','skill')]
skills_long <- skills_long[complete.cases(skills_long),]
```
skills_long dataframe has the information we need to calculate median salary by skill. Now we'll just add the data science skills categorizations from our keyword_count df above, and remove hourly wages. 
```{r}
skills_final <- merge(x=skills_long,y=keyword_count,by.x=c('skill')
                         ,by.y=c('keyword'))

skills_final <- skills_final[c('skill','salary_from_adjusted','category')]
skills_final <- skills_final %>%
  dplyr::filter(salary_from_adjusted>0) %>%
  dplyr::group_by(skill,category) %>%
  dplyr::summarise(median_starting_salary = median(salary_from_adjusted))
  
```
Graphing all 47 skills mentioned in the job postings would make an overly crowded chart. Therefore we will a) Make a combined chart showing the top 10 skills across categories by salary; B) Display all skills by first grouping them by skill category before charting. Even after grouping, a cowplot is not big enough to make the skill labels read-able, therefore we will print them individually. The knitted R Pubs document will place them in one container for easier comparison.
```{r}
top_salary_skills <- skills_final |> filter (median_starting_salary > 80000) |> dplyr::arrange(median_starting_salary) |> ggplot(aes(x= reorder(skill,median_starting_salary),y=median_starting_salary, fill = category)) + geom_bar(stat="identity", show.legend = TRUE) + xlab("Data Science Skill") + ylab("Median Starting Salary of Job Requiring Skill") + labs(title = "Top Skills by Base Salary in NYC Job Listings", caption = "(Based on data from NYC OpenData)") + guides(fill=guide_legend(title="Skill Category")) + coord_flip() + scale_y_continuous(labels = scales::label_dollar(scale=0.001,suffix="K")) + geom_text(aes(label=dollar_format(scale=0.001,suffix="K")(median_starting_salary)))

skills_acq_graph <- skills_final |> filter(category == 'Data Acquisition') |> dplyr::arrange(median_starting_salary) |> ggplot(aes(x= reorder(skill,median_starting_salary),y=median_starting_salary, fill = category)) + geom_bar(stat="identity", show.legend = TRUE,fill='#f8766d') + xlab("Data Science Skill") + ylab("Median Starting Salary of Job Requiring Skill") + labs(title = "Data Acquisition Skills by Base Salary in NYC Job Listings", caption = "(Based on data from NYC OpenData)") + guides(fill=guide_legend(title="Skill Category")) + coord_flip() + scale_y_continuous(labels = scales::label_dollar(scale=0.001,suffix="K")) + geom_text(aes(label=dollar_format(scale=0.001,suffix="K")(median_starting_salary)))

skills_analysis_graph <- skills_final |> filter(category == 'Data Analysis') |> dplyr::arrange(median_starting_salary) |> ggplot(aes(x= reorder(skill,median_starting_salary),y=median_starting_salary, fill = category)) + geom_bar(stat="identity", show.legend = TRUE,fill='#a3a500') + xlab("Data Science Skill") + ylab("Median Starting Salary of Job Requiring Skill") + labs(title = "Data Analysis Skills by Base Salary in NYC Job Listings", caption = "(Based on data from NYC OpenData)") + guides(fill=guide_legend(title="Skill Category")) + coord_flip() + scale_y_continuous(labels = scales::label_dollar(scale=0.001,suffix="K")) + geom_text(aes(label=dollar_format(scale=0.001,suffix="K")(median_starting_salary)))

skills_exploration_graph <- skills_final |> filter(category == 'Data Exploration') |> dplyr::arrange(median_starting_salary) |> ggplot(aes(x= reorder(skill,median_starting_salary),y=median_starting_salary, fill = category)) + geom_bar(stat="identity", show.legend = TRUE,fill='#00bf7d') + xlab("Data Science Skill") + ylab("Median Starting Salary of Job Requiring Skill") + labs(title = "Data Exploration Skills by Base Salary in NYC Job Listings", caption = "(Based on data from NYC OpenData)") + guides(fill=guide_legend(title="Skill Category")) + coord_flip() + scale_y_continuous(labels = scales::label_dollar(scale=0.001,suffix="K")) + geom_text(aes(label=dollar_format(scale=0.001,suffix="K")(median_starting_salary)))

skills_preparation_graph <- skills_final |> filter(category == 'Data Preparation') |> dplyr::arrange(median_starting_salary) |> ggplot(aes(x= reorder(skill,median_starting_salary),y=median_starting_salary, fill = category)) + geom_bar(stat="identity", show.legend = TRUE,fill='#00b0f6') + xlab("Data Science Skill") + ylab("Median Starting Salary of Job Requiring Skill") + labs(title = "Data Preparation Skills by Base Salary in NYC Job Listings", caption = "(Based on data from NYC OpenData)") + guides(fill=guide_legend(title="Skill Category")) + coord_flip() + scale_y_continuous(labels = scales::label_dollar(scale=0.001,suffix="K")) + geom_text(aes(label=dollar_format(scale=0.001,suffix="K")(median_starting_salary)))

skills_visualization_graph <- skills_final |> filter(category == 'Data Visualization') |> dplyr::arrange(median_starting_salary) |> ggplot(aes(x= reorder(skill,median_starting_salary),y=median_starting_salary, fill = category)) + geom_bar(stat="identity", show.legend = TRUE,fill='#e76bf3') + xlab("Data Science Skill") + ylab("Median Starting Salary of Job Requiring Skill") + labs(title = "Data Visualization Skills by Base Salary in NYC Job Listings", caption = "(Based on data from NYC OpenData)") + guides(fill=guide_legend(title="Skill Category")) + coord_flip() + scale_y_continuous(labels = scales::label_dollar(scale=0.001,suffix="K")) + geom_text(aes(label=dollar_format(scale=0.001,suffix="K")(median_starting_salary)))
```

There are 12 skills which offer a median starting salary of $85,000 or more in the NYC job listings. Perhaps surprisingly, only one "Data Visualization" skill is in the top 12, tied for #1: "UX design." In general though, a mix of skill categories make up the top 12. Top paying skills by category: 
- Data Acquisition: "JSON" - $100K
- Data Analysis: "Data Modeling" - $95K
- Data Exploration: "Data Profiling" - $100K
- Data preparation: "Data Formatting" - $95K
- Data Visualization: "User Experience Design" - $100K

If one was interested in pursuing an NYC government job, and was particularly interested in one of these categories, they could use these data as a reference point on where to invest their time. 

```{r}
#Printing charts
top_salary_skills
skills_acq_graph
skills_analysis_graph
skills_exploration_graph
skills_preparation_graph
skills_visualization_graph
```


Skill Analysis Conclusions
- Of the 3,261 jobs listed on the NYC government site, 57.5% required a data science skill.
- There were 47 individual data science skills mentioned on the NYC jobs postings.
- The most in-demand data science skills were "database" (459 mentions) followed by "data entry" (261), "data analysis" (214), "SQL" (193), and "data collection" (117). Four of these top five fall under the data science workflow category of "Data Acquisition." 
- While 57.5% of all job posting required a data science skill, those jobs requested a varied array of skills. The three most in-demand skills, "database," "data entry," and "data analysis," were found in 14.1%, 8.0%, and 6.6% of all job postings. 
- The top 12 skills by base salary came from all the data science workflow categories: 4 data preparation skills made up the top 12, 4 data acquisition skills, 2 data analysis skills, and 1 skill each from data exploration and data visualization.
