---
title: "Project 3"
author: "Alice, Farhana, Nick, Ross, and Taha,"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Overview

The dataset our team has chosen to use for this project is a table of job postings on https://data.cityofnewyork.us/ provided by the Department of Citywide Administrative Services (DCAS). A description taken from the [source](https://dev.socrata.com/foundry/data.cityofnewyork.us/kpav-sd4t) is below:

```This dataset contains current job postings available on the City of New York’s official jobs site (http://www.nyc.gov/html/careers/html/search/search.shtml). Internal postings available to city employees and external postings available to the general public are included.```

The original table is comprised of 30 columns, but in order to make the data more digestible and easier to read, we've broken this table down into 4 smaller ones; below is an ERD of our relational database model. 

![ERD](https://raw.githubusercontent.com/alu-potato/DATA607Project3/main/erd/Project3ERD2.png)

### Relational Database Setup

For our project, we are utilizing a relational database hosted in Azure. Specifically it is an instance of Azure SQL.

#### Setting Up a Keyring and Creating the Connection

The first step to setting up our MySQL database is to connect to it. Here we utilize the odbc package in combination with the dbi package to connect to our cloud hosted instance of Azure SQL. To keep our SQL Database secure, We also utilize the keyring package in order to ensure that the password never gets stored as code. Please note that to connect through odbc to Azure SQL Server, OBDC Driver 18 for SQL Server is required. This can be downloaded here: https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server?view=sql-server-ver16.

```{r connect to Azure SQL Server, message = FALSE}
library(tidyverse)
library(odbc)
library(DBI)
library(keyring)
if (!("Overview8909" %in% as_vector(key_list()[2]))) {
  key_set("Project3SQL","Overview8909")
}
```

#### Configuring Database Tables

As this is a fresh Azure SQL server instance, we want to create the tables that will hold the information of our dataset. We initialize the tables with queries built to match the contents of each table drafted in the ERD.

```{r Configuring Database Tables, eval = FALSE}
query1 <- r"(
  CREATE TABLE nyc_jobs_codes( 
    title_code_num VARCHAR(10),
    civil_service_title VARCHAR(40),
    title_classification VARCHAR(30),
    level VARCHAR(5),
    PRIMARY KEY (title_code_num));
)"

query2 <- r"(
  CREATE TABLE nyc_jobs_agencies( 
    work_unit VARCHAR(40),
    agency VARCHAR(40),
    agency_location VARCHAR(30),
    PRIMARY KEY (work_unit));
)"

query3 <- r"(
  CREATE TABLE nyc_jobs_specifics(
    job_id INT,
    posting_type VARCHAR(12),
    job_description VARCHAR(MAX),
    minimum_qual VARCHAR(MAX),
    preferred_skills VARCHAR(4000),
    additional_information VARCHAR(3500),
    to_apply VARCHAR(3500),
    shift VARCHAR(2000),
    work_location VARCHAR(1000),
    recruitment_contact VARCHAR(10),
    residency_requirement VARCHAR(1000),
    post_until VARCHAR(15),
    process_date VARCHAR(15),
    PRIMARY KEY (job_id));
)"

query4 <- r"(
  CREATE TABLE nyc_jobs_basics( 
    job_id INT,
    num_positions INT,
    business_title VARCHAR(150),
    title_code_num VARCHAR(10),
    level VARCHAR(5),
    job_category VARCHAR(300),
    full_time_indicator VARCHAR(2),
    career_level VARCHAR(30),
    salary_range_from DEC(20,10),
    salary_range_to DEC(20,10),
    salary_frequency VARCHAR(10),
    work_unit VARCHAR(40),
    posting_date VARCHAR(15),
    posting_updated VARCHAR(15),
    FOREIGN KEY (title_code_num) REFERENCES nyc_jobs_codes(title_code_num), 
    FOREIGN KEY (work_unit) REFERENCES nyc_jobs_agencies(work_unit),
    FOREIGN KEY (job_id) REFERENCES nyc_jobs_specifics(job_id),
    PRIMARY KEY (job_id));
)"

my_connection <- dbConnect(drv = odbc::odbc(),
                           Driver = "ODBC Driver 18 for SQL Server",
                           server = "tcp:data607project3server.database.windows.net,1433",
                           database = "Data 607 Project 3 Database",
                           uid = "Overview8909",
                           pwd = key_get("Project3SQL","Overview8909"),
                           encoding = "latin1"
                           )

query_combination <- c(query1, query2, query3, query4)
for (query in query_combination) {
  dbExecute(my_connection,query)
  Sys.sleep(0.5)
}

dbDisconnect(my_connection)
```

#### Populating Database tables
We first load the dataset into R, preprocess the data, and then store the data from our dataframes into the database tables.

```{r Populating Database Tables, eval = FALSE}
url <- r"(https://data.cityofnewyork.us/api/views/kpav-sd4t/rows.csv?accessType=DOWNLOAD)"
nyc_jobs_raw <- read_csv(url, show_col_types = FALSE)
colnames(nyc_jobs_raw) <- c(
    'job_id',
    'agency',
    'posting_type',
    'num_positions',
    'business_title',
    'civil_service_title',
    'title_classification',
    'title_code_num',
    'level',
    'job_category',
    'full_time_indicator',
    'career_level',
    'salary_range_from',
    'salary_range_to',
    'salary_frequency',
    'agency_location',
    'work_unit',
    'job_description',
    'minimum_qual',
    'preferred_skills',
    'additional_information',
    'to_apply',
    'shift',
    'work_location',
    'recruitment_contact',
    'residency_requirement',
    'posting_date',
    'post_until',
    'posting_updated',
    'process_date')

nyc_jobs_raw[is.na(nyc_jobs_raw)] <- "" 
nyc_jobs_raw <- nyc_jobs_raw[!duplicated(nyc_jobs_raw$job_id),]
nyc_jobs_raw <- nyc_jobs_raw |> 
    mutate_all(~ ifelse(. == "", NA, .)) |> 
    mutate(job_description = str_to_title(str_replace_all(job_description, "[^[:alnum:][:space:]]", ""))) |>
    mutate(job_description = str_replace_all(job_description, "â", "'")) |> 
    mutate(job_description = str_remove_all(job_description, "\\t|Â")) |> 
    mutate(preferred_skills = str_to_title(str_replace_all(preferred_skills, "[^[:alnum:][:space:]]", ""))) |> 
    mutate(preferred_skills = str_remove_all(preferred_skills, "Â\\t|\\t|Â|â")) |> 
    mutate(minimum_qual = str_replace_all(minimum_qual, "â|\u0080|\u0099", "")) |>
    mutate(salary_range_from = round(salary_range_from,2)) |>
    mutate(salary_range_to = round(salary_range_to,2))

nyc_jobs_codes <- nyc_jobs_raw |> #Use this as this an outline for each new dataframe table
  select(c('civil_service_title','title_classification','level','title_code_num'))
nyc_jobs_codes <- nyc_jobs_codes[!duplicated(nyc_jobs_codes$title_code_num),]
nyc_jobs_agencies <- nyc_jobs_raw |> 
  select(c('agency', 'agency_location', 'work_unit'))
# nyc_jobs_agencies <- nyc_jobs_agencies[!duplicated(nyc_jobs_agencies$work_unit),]
nyc_jobs_agencies <- nyc_jobs_agencies[!duplicated(tolower(nyc_jobs_agencies$work_unit)),]
nyc_jobs_agencies
nyc_jobs_specifics <- nyc_jobs_raw |> 
  select(c('job_id', 'posting_type', 'job_description', 'minimum_qual', 'preferred_skills', 'additional_information', 'to_apply', 'shift', 'work_location', 'recruitment_contact', 'residency_requirement', 'post_until', 'process_date'))
nyc_jobs_specifics <- nyc_jobs_specifics[!duplicated(nyc_jobs_specifics$job_id),]
nyc_jobs_basics <- nyc_jobs_raw |> 
  select(c('job_id', 'num_positions', 'business_title', 'title_code_num', 'level', 'job_category', 'full_time_indicator', 'career_level', 'salary_range_from', 'salary_range_to', 'salary_frequency', 'work_unit', 'posting_date', 'posting_updated'))
nyc_jobs_basics <- nyc_jobs_basics[!duplicated(nyc_jobs_basics$job_id),]  

my_connection <- dbConnect(drv = odbc::odbc(),
                           Driver = "ODBC Driver 18 for SQL Server",
                           server = "tcp:data607project3server.database.windows.net,1433",
                           database = "Data 607 Project 3 Database",
                           uid = "Overview8909",
                           pwd = key_get("Project3SQL","Overview8909"),
                           encoding = "latin1"
                           )

dbAppendTable(my_connection,"nyc_jobs_codes", nyc_jobs_codes)
Sys.sleep(0.5)
dbAppendTable(my_connection,"nyc_jobs_agencies", nyc_jobs_agencies)
Sys.sleep(0.5)
dbAppendTable(my_connection,"nyc_jobs_specifics", nyc_jobs_specifics) 
Sys.sleep(0.5)
dbAppendTable(my_connection,"nyc_jobs_basics", nyc_jobs_basics)
Sys.sleep(0.5)

dbDisconnect(my_connection)
```
 
 
### Exploration


To keep things relevant, here are the fields we'll be going through as part of this exploratory analysis:
- `nyc_jobs_basics.job_id`: The job opening identification (“Job ID”) number that corresponds to and represents a job posting notice published on behalf of a New York City agency. 
- `nyc_jobs_basics.work_unit` (agency): Name of the New York City agency (“agency” or “hiring agency”) where a job vacancy exists.
- `nyc_jobs_specifics.posting_type`: Identifies whether a job posting is an Internal or External posting. Internal postings are available to City employees only and external postings are available to the general public.
- `nyc_jobs_basics.job_category` : The occupational group in which the posted job belongs, such as: Administration & Human Resources; Communications & Intergovernmental Affairs; Constituent Services & Community Programs; Engineering, Architecture, & Planning; Finance, Accounting, & Procurement; Health; Technology, Data & Innovation; Legal Affairs; Building Operations & Maintenance; Policy, Research & Analysis; Public Safety, Inspections, & Enforcement; Social Services
- `nyc_jobs_basics.full_time_indicator`: "This denotes whether the job is a full time or part time employment; F - Full time; P - Part time"
- `nyc_jobs_basics.career_level`: "This denotes the career level of the job. The possible career levels are:; Student; Entry-level; Experienced (non-manager); Manager; Executive"
- `nyc_jobs_basics.salary_range_from`: The lowest salary on a job posting for a position within the salary band for the related civil service title.
- `nyc_jobs_basics.salary_range_to`: The highest salary on a job posting for a position within the salary band for the related civil service title.
- `nyc_jobs_basics.salary_frequency`: The frequency of proposed salary.  Possible salary frequency values include “hourly”, “daily”, and “annual”. 
- `nyc_jobs_basics.posting_date`: The date and time that a job vacancy was posted in MM/DD/YY format.

First, we load in additional libraries that will be used for the data exploration.

```{r additional library, message = FALSE}
library(plyr)
library(ggplot2)
```

We must then connect to our database and create dataframes in R that represent each table we are going to be working with.

```{r pull_data}
my_connection <- dbConnect(drv = odbc::odbc(),
                           Driver = "ODBC Driver 18 for SQL Server",
                           server = "tcp:data607project3server.database.windows.net,1433",
                           database = "Data 607 Project 3 Database",
                           uid = "Overview8909",
                           pwd = key_get("Project3SQL","Overview8909"),
                           encoding = "latin1"
                           )

nyc_jobs_basics_sql <- "select * from nyc_jobs_basics"
nyc_jobs_agencies_sql <- "select * from nyc_jobs_agencies"
nyc_jobs_codes_sql <- "select * from nyc_jobs_codes"
nyc_jobs_specifics_sql <- "select * from nyc_jobs_specifics"

nyc_jobs_basics <- dbGetQuery(my_connection, nyc_jobs_basics_sql)
nyc_jobs_agencies <- dbGetQuery(my_connection, nyc_jobs_agencies_sql)
nyc_jobs_codes <- dbGetQuery(my_connection, nyc_jobs_codes_sql)
nyc_jobs_specifics <- dbGetQuery(my_connection, nyc_jobs_specifics_sql)

dbDisconnect(my_connection)
```

To start, we'll first just get an overall count of the data.

```{r counts}
overall_count <- nyc_jobs_basics |> 
  dplyr::summarise(count = n())

sprintf("There are %i jobs currently posted.", overall_count$count)
```

As of Friday, March 17, there are 3,260 jobs currently available. 

How can we break this down further?

#### Agency Posted

What agencies are posting these jobs?

```{r agency}
agency_count <- nyc_jobs_basics |> 
  dplyr::summarise(count = n_distinct(work_unit))

sprintf("There are %i agencies.", agency_count$count)

counts_by_agency <- nyc_jobs_basics |> 
  group_by(work_unit) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

head(counts_by_agency, 10)
```

In total, there are 1,048 different agencies and no agency is more prevalent in job postings than another. The fact that the largest one only has 44 current job postings out of 3k+ (\~1%) shows that it's an extremely diverse collection of jobs.


### Posting Type

Are a majority of these listings internal or external?

```{r posting_type}
counts_by_posting_type <- nyc_jobs_specifics |> 
  group_by(posting_type) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))
head(counts_by_posting_type)
```

Pretty close to 50/50, although internal is slightly higher at 55% vs. external's 45%.

#### Job Category

What job categories do these postings fall under?

```{r job_category}
category_count <- nyc_jobs_basics |> 
  dplyr::summarise(count = n_distinct(job_category))

sprintf("There are %i job categories", category_count$count)

counts_by_job_category <- nyc_jobs_basics |> 
  group_by(job_category) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

head(counts_by_job_category, 10)
```

In total, there are 192 distinct job categories. Of these, we can see that the most common categories are `Engineering, Architecture, & Planning` (\~13%), `Technology, Data, & Innovation` (\~8%), `Legal Affairs` (7%), and `Administration & Human Resources` (\~6%). 

Circling back to our goal of finding data science related jobs and skills, one way to hone in on this is to use category as a way to see what how many of these postings fall into categories with the words `analysis`, `analytics`, `data`, or `statistics`? 

```{r relevant_categories}
to_find <- c("Data", "Analysis", "Analytics", "Statistics")
matches <- unique(grep(paste(to_find, collapse="|"), nyc_jobs_basics$job_category, value=TRUE))

relevant_categories <- filter(nyc_jobs_basics, job_category %in% matches)

overall_relevant_count <- relevant_categories |> 
  dplyr::summarise(count = n())

counts_by_relevant_job_category <- relevant_categories |> 
  group_by(job_category) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(relevant_categories),
            percent_of_total = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

sprintf("There are %i jobs currently posted within a relevant category.", overall_relevant_count$count)

head(counts_by_relevant_job_category, 20)
```

Here, we can see there are 784 jobs currently posted with a category containing one of our keywords with `Policy, Research & Analysis` bubbling up as the second most popular relevant category at 85 postings (\~11% of this subset, \~3% of total).

#### Full/Part Time

What is the breakdown of these jobs by full vs. part time?

```{r full_part_time}
counts_by_full_part_time <- nyc_jobs_basics |> 
  group_by(full_time_indicator) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

head(counts_by_full_part_time)
```

It seems like there are a few rows where this isn't filled out, but a vast majority of the jobs posted are full-time listings.

#### Career Level

What is the breakdown of these jobs by career level?

```{r career_level}
counts_by_career_level <- nyc_jobs_basics |> 
  group_by(career_level) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

head(counts_by_career_level)
```

Interestingly, 72% (2.3k) of these jobs are for `Experienced (non-manager)` roles. 

#### Salary Information

How do these jobs look in terms of salaray?

##### Frequency

We can start with frequency -- how often are folks paid for these roles?

```{r salary_frequency}
counts_by_salary_frequency <- nyc_jobs_basics |> 
  group_by(salary_frequency) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

head(counts_by_salary_frequency)
```

A vast majority at \~89% (2.9k) are annual, although in order to properly compare compensation, we'll have to adjust the hourly and daily rates up to an annual value. We'll assume 40 hour weeks and 52 weeks in a year for the hourly folks and 365 days a year for the daily ones. I'll also round these adjusted salary ranges to the nearest $10,000.

```{r adjust_salary_frequency}
salary_from_adjusted <- c()
salary_to_adjusted <- c()

freq <- ""

for(i in 1:nrow(nyc_jobs_basics)) {
    freq <- nyc_jobs_basics$salary_frequency[i]
    if (freq == "Annual") {
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics$salary_range_from[i], 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics$salary_range_to[i], 10000))
    } else if (freq == "Hourly") {
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics$salary_range_from[i] * 40 * 52, 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics$salary_range_to[i] * 40 * 52, 10000))
    } else { # this means it's daily
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics$salary_range_from[i] * 365, 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics$salary_range_to[i] * 365, 10000))
    }
}

nyc_jobs_basics$salary_from_adjusted <- salary_from_adjusted
nyc_jobs_basics$salary_to_adjusted <- salary_to_adjusted

head(nyc_jobs_basics)
```

Now with this, how do the salary ranges look? Let's start with the beginning band (from).

```{r salary_from_chart}
count_by_salary_from <- nyc_jobs_basics |> 
  group_by(salary_from_adjusted, salary_frequency) |>
  dplyr::summarise(count = n(),
                   percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

salary_from_bar <-ggplot(data=count_by_salary_from
                   , aes(x=salary_from_adjusted
                         , y=count)) + 
  geom_bar(stat="identity", position=position_dodge()) + 
  scale_fill_brewer(palette="Paired")+
  theme_minimal()

salary_from_summary <- nyc_jobs_basics |>
  dplyr::summarise(mean = mean(salary_from_adjusted),
                   median = median(salary_from_adjusted),
                   min = min(salary_from_adjusted),
                   max = max(salary_from_adjusted))

salary_from_summary

salary_from_bar
```

The data looks to be right skewed here with a center around \~\$60k and a high of \~\$230k. The mean and median are close but not quite the same so it's not a perfectly normal distribution. What about the to (high) portion of the salary range?


```{r salary_to_chart}
count_by_salary_to <- nyc_jobs_basics |> 
  group_by(salary_to_adjusted, salary_frequency) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

salary_to_bar <-ggplot(data=count_by_salary_to
                   , aes(x=salary_to_adjusted
                         , y=count)) + 
  geom_bar(stat="identity", position=position_dodge()) + 
  scale_fill_brewer(palette="Paired")+
  theme_minimal()

salary_to_summary <- nyc_jobs_basics |>
  dplyr::summarise(mean = mean(salary_to_adjusted),
                   median = median(salary_to_adjusted),
                   min = min(salary_to_adjusted),
                   max = max(salary_to_adjusted))

salary_to_summary

salary_to_bar
```
This one looks less like a bell-curve (still right-skewed though) and seems to have a high frequency at \~\$60k as well, however there seems to be more jobs with rates at the tail ends of the spectrum. We can see that the max here goes up to \~\$250k with more activity in the \~\$100k+ range. The mean and median here have been brought up though at \$88k and \$80k respectively while the minimum is also at \$30k; in general, these rates pay more as they're the at the higher end of the spectrum.

What does salary look like if we assume most folks get the middle point of each posted range?

```{r salary_avg_chart}
nyc_jobs_basics$salary_mid_adjusted <- (nyc_jobs_basics$salary_to_adjusted + nyc_jobs_basics$salary_from_adjusted) / 2

count_by_salary_mid <- nyc_jobs_basics |> 
  group_by(salary_mid_adjusted, salary_frequency) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

salary_mid_bar <-ggplot(data=count_by_salary_mid
                   , aes(x=salary_mid_adjusted
                         , y=count)) + 
  geom_bar(stat="identity", position=position_dodge()) + 
  scale_fill_brewer(palette="Paired")+
  theme_minimal()

salary_mid_summary <- nyc_jobs_basics |>
  dplyr::summarise(mean = mean(salary_mid_adjusted),
                   median = median(salary_mid_adjusted),
                   min = min(salary_mid_adjusted),
                   max = max(salary_mid_adjusted))

salary_mid_summary

salary_mid_bar
```
Using the mid-point, we can see that the curve now peaks at \~\$65k. It's still a bit right-skewed with a tail trailing off at \~\$230k as well while the mean and median now are closer at ~\$\77k and \~\$75k respectively..

#### Post Date

In this data set, when were these jobs posted?

```{r post_date}
nyc_jobs_basics$posting_date <- as.Date(nyc_jobs_basics$posting_date, "%m/%d/%Y")

count_by_date <- nyc_jobs_basics |> 
  group_by(year = format(posting_date, '%Y')) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

head(count_by_date, 10)
```

It looks like a majority of these job listings are from this year and last year, however some of them are even from 2014. For the most part though, it seems like they're mostly recent as 48% of them are from this year, but it's only been \~3.5 months as it's only March.

### Data Analysis

Now we want to work towards the main question of this data analysis: "Which are the most valued data science skills?" The approach that we are beginning with is breaking down data science keywords which are present within the job description, minimum qualification, and preferred skills columns. If we can figure out which data science key words appear the most, then we could attempt to determine which skills corresponding to these keywords are most in demand.

#### Querying Data

As we only need a subset of data from two tables for our analysis, we will go ahead and run a SQL query to retrieve the columns that we want.

```{r get combined tables}

my_connection <- dbConnect(drv = odbc::odbc(),
                           Driver = "ODBC Driver 18 for SQL Server",
                           server = "tcp:data607project3server.database.windows.net,1433",
                           database = "Data 607 Project 3 Database",
                           uid = "Overview8909",
                           pwd = key_get("Project3SQL","Overview8909"),
                           encoding = "latin1"
                           )

nyc_jobs_combination_sql <- r"(
SELECT b.job_id, b.business_title, b.job_category, s.job_description, s.minimum_qual, s.preferred_skills
FROM nyc_jobs_basics as b 
JOIN nyc_jobs_specifics as s ON b.job_id = s.job_id
)"

nyc_jobs_combination <- dbGetQuery(my_connection, nyc_jobs_combination_sql)

dbDisconnect(my_connection)

nyc_jobs_combination[,4:6] <- lapply(nyc_jobs_combination[,4:6], tolower)

head(nyc_jobs_combination)
```

#### High Level Look at Keywords

We want to search our dataframe for specific data science keywords broken down into categories:

* Data Acquisition keywords: SQL, "data acquisition", databases, "cloud storage", APIs, “data entry”, etc.
* Data Preparation keywords: Alteryx, Knime, Excel, "data engineering", "preprocessing", "data preparation", etc.
* Data Exploration keywords: Tableau, Excel, "data analysis", "feature selection", "data engineering", “data exploration”, etc.
* Data Analysis keywords:  Excel, "data modeling", "machine learning", “data analysis”, "artificial intelligence", etc.
* Data Visualization/Communication: Tableau, PowerBI, "data insights", "quantitative findings", “graph creation”, etc.

To begin this analysis we can look at the count of keywords within the relevant dataframe columns as a whole. Although, this new dataframe can not be used for much further analyis, it would provide us a simple answer to which data science skills are most in demand based on this dataset.

```{r detect keywords}
library(stringi)

search_keywords <- function(df, keywords, category) {
  # Create a vector to store the counts of each keyword
  word_count <- rep(0, length(keywords))
  
  # Loop through each row of the dataframe
  for (i in 1:nrow(df)) {
    # Convert all the text in each column to lowercase and combine them into a single string
    text <- paste(df[i, ], collapse = " ")
    # Loop through each keyword in the keyword list
    for (j in 1:length(keywords)) {
      # If the keyword is in the text, increment the count for that keyword
        # ignore.case to make sure either "Python" or "python" is counted
      if (stri_detect_regex(text, paste0("\\b", keywords[j], "\\b"))) {
        word_count[j] <- word_count[j] + 1
      }
    }
  }
  
  # Return the word counts as a tibble
  keywords_count <- tibble(
    category = category,
    keyword = keywords,
    count = word_count
    )
  return(keywords_count)
}

data_acq_keys <- tolower(
  c("SQL", "data acquisition", "database", "cloud storage", "API", "data entry", "data scraping", "scraping", "mongodb", "postgre", "mariadb", "oracle", "signal reception", "data extraction", "web scraping", "etl", "data collection", "web crawling", "database management", "data warehouse" )
)
data_prep_keys <- tolower(
  c("Alteryx", "Knime", "Excel", "data engineering", "preprocessing", "data preparation", "data cleaning", "data wrangling", "data transformation", "data integration", "normalization", "imputation", "data formatting", "data merging", "data enrichment")
)
data_exp_keys <- tolower(
  c("Tableau", "Excel", "data analysis", "feature selection", "data engineering", "data exploration", "EDA", "Descriptive Statistics", "Data Profiling", "Data Quality Assessment", "Data Sampling", "Dimensionality Reduction", "Feature Engineering", "Correlation Analysis", "Outlier Detection", "Pattern Identification")
)
data_anal_keys <- tolower(
  c("Excel", "data modeling", "machine learning", "data analysis", "artificial intelligence", "qualitative analysis", "predictive analysis", "regression", "Statistical Analysis", "Classification", "Clustering", "Time Series", "Hypothesis Testing", "A/B Testing", "Data Mining" )
)
data_vis_keys <- tolower(
  c("Tableau", "PowerBI", "data insights", "quantitative findings", "graph creation", "data reporting", "data visualization", "busines intelligence", "Data Storytelling", "Data Presentation","Information Design", "Dashboard Creation", "Charting","Infographics", "Data Visualization Tools", "Interactive Data Visualization")
)

data_science_categories <- list(
  data_acquisition = data_acq_keys, 
  data_preparation = data_prep_keys,
  data_exploration = data_exp_keys, 
  data_analysis = data_anal_keys, 
  data_visualization = data_vis_keys
)

keyword_count <- rbind(
  search_keywords(nyc_jobs_combination, data_acq_keys, "Data Acquisition"),
  search_keywords(nyc_jobs_combination, data_prep_keys, "Data Preparation"),
  search_keywords(nyc_jobs_combination, data_exp_keys, "Data Exploration"),
  search_keywords(nyc_jobs_combination, data_anal_keys, "Data Analysis"),
  search_keywords(nyc_jobs_combination, data_vis_keys, "Data Visualization")
)

head(arrange(keyword_count,desc(count)), n = 10)

```
If we look at the individual keywords themselves, it may come as no surprise that Excel is the number one skill of data scientists in demand. As most office jobs require Excel in some form or the other. We can also see which data science category itself ends up coming on top.

```{r high level category count}

keyword_count |>
  group_by(category) |>
  summarise(count = sum(count)) |>
  arrange(count) |>
  ggplot(aes(x= reorder(category,count),y=count, fill = category))+
    geom_bar(stat="identity", show.legend = FALSE) +
    xlab("Data Science Category") +
    ylab("Job Listing Count") +
    labs(title = "Data Science Categories in NY Job Listings", caption = "(Based on data from NYC OpenData)") +
    geom_text(aes(label=count)) +
    coord_flip()

```

#### Mutating Data Based on Keywords

In order to be able to truly relate the keywords within each column to other information within the dataset, we will need to create new columns on the dataset that detects matches for each data science skill category. The number of matches is stored as an integer for each column. We then also create a total matches that can perhaps be used to measure how relevant a job is to data science.

```{r keyword mutation}
# Create the new columns in the data frame which count up keyword matches within each category
for (category_name in names(data_science_categories)) {
  nyc_jobs_combination[[category_name]] <- as_tibble(sapply(data_science_categories[[category_name]],grepl,apply(nyc_jobs_combination[, 4:6], 1, paste, collapse = " "))) %>%
    mutate(!!category_name := as.integer(rowSums(across(where(is.logical)))), .keep="none") %>%
    pull(!!category_name)
}

# Create a column for total data science matches in the data frame
nyc_jobs_combination <- nyc_jobs_combination |>
  mutate(total_datascience_keywords = as.integer(rowSums(across(data_acquisition:data_visualization))))
```

#### Analyzing Categorical Association With Other Factors

## Conclusions

...