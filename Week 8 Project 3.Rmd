---
title: "Project 3"
author: "Alice, Farhana, Nick, Ross, and Taha,"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Overview

The dataset our team has chosen to use for this project is a table of job postings on https://data.cityofnewyork.us/ provided by the Department of Citywide Administrative Services (DCAS). A description taken from the [source](https://dev.socrata.com/foundry/data.cityofnewyork.us/kpav-sd4t) is below:

```This dataset contains current job postings available on the City of New York’s official jobs site (http://www.nyc.gov/html/careers/html/search/search.shtml). Internal postings available to city employees and external postings available to the general public are included.```

The original table is comprised of 30 columns, but in order to make the data more digestible and easier to read, we've broken this table down into 4 smaller ones; below is an ERD of our relational database model. 

![ERD](https://raw.githubusercontent.com/alu-potato/DATA607Project3/main/erd/Project3ERD2.png)

### Relational Database Setup

For our project, we are utilizing a relational database hosted in Azure. Specifically it is an instance of Azure SQL.

#### Setting Up a Keyring and Creating the Connection

The first step to setting up our MySQL database is to connect to it. Here we utilize the odbc package in combination with the dbi package to connect to our cloud hosted instance of Azure SQL. To keep our SQL Database secure, We also utilize the keyring package in order to ensure that the password never gets stored as code. Please note that to connect through odbc to Azure SQL Server, OBDC Driver 18 for SQL Server is required. This can be downloaded here: https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server?view=sql-server-ver16.

```{r connect to Azure SQL Server, message = FALSE}
library(tidyverse)
library(odbc)
library(DBI)
library(keyring)
if (!("Overview8909" %in% as_vector(key_list()[2]))) {
  key_set("Project3SQL","Overview8909")
}
```

#### Configuring Database Tables

As this is a fresh Azure SQL server instance, we want to create the tables that will hold the information of our dataset. We initialize the tables with queries built to match the contents of each table drafted in the ERD.

```{r Configuring Database Tables, eval = FALSE}
query1 <- r"(
  CREATE TABLE nyc_jobs_codes( 
    title_code_num VARCHAR(10),
    civil_service_title VARCHAR(40),
    title_classification VARCHAR(30),
    level VARCHAR(5),
    PRIMARY KEY (title_code_num));
)"

query2 <- r"(
  CREATE TABLE nyc_jobs_agencies( 
    work_unit VARCHAR(40),
    agency VARCHAR(40),
    agency_location VARCHAR(30),
    PRIMARY KEY (work_unit));
)"

query3 <- r"(
  CREATE TABLE nyc_jobs_specifics(
    job_id INT,
    posting_type VARCHAR(12),
    job_description VARCHAR(MAX),
    minimum_qual VARCHAR(MAX),
    preferred_skills VARCHAR(4000),
    additional_information VARCHAR(3500),
    to_apply VARCHAR(3500),
    shift VARCHAR(2000),
    work_location VARCHAR(1000),
    recruitment_contact VARCHAR(10),
    residency_requirement VARCHAR(1000),
    post_until VARCHAR(15),
    process_date VARCHAR(15),
    PRIMARY KEY (job_id));
)"

query4 <- r"(
  CREATE TABLE nyc_jobs_basics( 
    job_id INT,
    num_positions INT,
    business_title VARCHAR(150),
    title_code_num VARCHAR(10),
    level VARCHAR(5),
    job_category VARCHAR(300),
    full_time_indicator VARCHAR(2),
    career_level VARCHAR(30),
    salary_range_from DEC(20,10),
    salary_range_to DEC(20,10),
    salary_frequency VARCHAR(10),
    work_unit VARCHAR(40),
    posting_date VARCHAR(15),
    posting_updated VARCHAR(15),
    FOREIGN KEY (title_code_num) REFERENCES nyc_jobs_codes(title_code_num), 
    FOREIGN KEY (work_unit) REFERENCES nyc_jobs_agencies(work_unit),
    FOREIGN KEY (job_id) REFERENCES nyc_jobs_specifics(job_id),
    PRIMARY KEY (job_id));
)"

my_connection <- dbConnect(drv = odbc::odbc(),
                           Driver = "ODBC Driver 18 for SQL Server",
                           server = "tcp:data607project3server.database.windows.net,1433",
                           database = "Data 607 Project 3 Database",
                           uid = "Overview8909",
                           pwd = key_get("Project3SQL","Overview8909"),
                           encoding = "latin1"
                           )

query_combination <- c(query1, query2, query3, query4)
for (query in query_combination) {
  dbExecute(my_connection,query)
  Sys.sleep(0.5)
}

dbDisconnect(my_connection)
```

#### Populating Database tables
We first load the dataset into R, preprocess the data, and then store the data from our dataframes into the database tables.

```{r Populating Database Tables, eval = FALSE}
url <- r"(https://data.cityofnewyork.us/api/views/kpav-sd4t/rows.csv?accessType=DOWNLOAD)"
nyc_jobs_raw <- read_csv(url, show_col_types = FALSE)
colnames(nyc_jobs_raw) <- c(
    'job_id',
    'agency',
    'posting_type',
    'num_positions',
    'business_title',
    'civil_service_title',
    'title_classification',
    'title_code_num',
    'level',
    'job_category',
    'full_time_indicator',
    'career_level',
    'salary_range_from',
    'salary_range_to',
    'salary_frequency',
    'agency_location',
    'work_unit',
    'job_description',
    'minimum_qual',
    'preferred_skills',
    'additional_information',
    'to_apply',
    'shift',
    'work_location',
    'recruitment_contact',
    'residency_requirement',
    'posting_date',
    'post_until',
    'posting_updated',
    'process_date')

nyc_jobs_raw[is.na(nyc_jobs_raw)] <- "" 
nyc_jobs_raw <- nyc_jobs_raw[!duplicated(nyc_jobs_raw$job_id),]
nyc_jobs_raw <- nyc_jobs_raw |> 
    mutate_all(~ ifelse(. == "", NA, .)) |> 
    mutate(job_description = str_to_title(str_replace_all(job_description, "[^[:alnum:][:space:]]", ""))) |>
    mutate(job_description = str_replace_all(job_description, "â", "'")) |> 
    mutate(job_description = str_remove_all(job_description, "\\t|Â")) |> 
    mutate(preferred_skills = str_to_title(str_replace_all(preferred_skills, "[^[:alnum:][:space:]]", ""))) |> 
    mutate(preferred_skills = str_remove_all(preferred_skills, "Â\\t|\\t|Â|â")) |> 
    mutate(minimum_qual = str_replace_all(minimum_qual, "â|\u0080|\u0099", "")) |>
    mutate(salary_range_from = round(salary_range_from,2)) |>
    mutate(salary_range_to = round(salary_range_to,2))

nyc_jobs_codes <- nyc_jobs_raw |> #Use this as this an outline for each new dataframe table
  select(c('civil_service_title','title_classification','level','title_code_num'))
nyc_jobs_codes <- nyc_jobs_codes[!duplicated(nyc_jobs_codes$title_code_num),]
nyc_jobs_agencies <- nyc_jobs_raw |> 
  select(c('agency', 'agency_location', 'work_unit'))
nyc_jobs_agencies <- nyc_jobs_agencies[!duplicated(tolower(nyc_jobs_agencies$work_unit)),]
nyc_jobs_agencies
nyc_jobs_specifics <- nyc_jobs_raw |> 
  select(c('job_id', 'posting_type', 'job_description', 'minimum_qual', 'preferred_skills', 'additional_information', 'to_apply', 'shift', 'work_location', 'recruitment_contact', 'residency_requirement', 'post_until', 'process_date'))
nyc_jobs_specifics <- nyc_jobs_specifics[!duplicated(nyc_jobs_specifics$job_id),]
nyc_jobs_basics <- nyc_jobs_raw |> 
  select(c('job_id', 'num_positions', 'business_title', 'title_code_num', 'level', 'job_category', 'full_time_indicator', 'career_level', 'salary_range_from', 'salary_range_to', 'salary_frequency', 'work_unit', 'posting_date', 'posting_updated'))
nyc_jobs_basics <- nyc_jobs_basics[!duplicated(nyc_jobs_basics$job_id),]  

my_connection <- dbConnect(drv = odbc::odbc(),
                           Driver = "ODBC Driver 18 for SQL Server",
                           server = "tcp:data607project3server.database.windows.net,1433",
                           database = "Data 607 Project 3 Database",
                           uid = "Overview8909",
                           pwd = key_get("Project3SQL","Overview8909"),
                           encoding = "latin1"
                           )

dbAppendTable(my_connection,"nyc_jobs_codes", nyc_jobs_codes)
Sys.sleep(0.5)
dbAppendTable(my_connection,"nyc_jobs_agencies", nyc_jobs_agencies)
Sys.sleep(0.5)
dbAppendTable(my_connection,"nyc_jobs_specifics", nyc_jobs_specifics) 
Sys.sleep(0.5)
dbAppendTable(my_connection,"nyc_jobs_basics", nyc_jobs_basics)
Sys.sleep(0.5)

dbDisconnect(my_connection)
```
 
 
## Data Exploration


To keep things relevant, here are the fields we'll be going through as part of this exploratory analysis:
- `nyc_jobs_basics.job_id`: The job opening identification (“Job ID”) number that corresponds to and represents a job posting notice published on behalf of a New York City agency. 
- `nyc_jobs_basics.work_unit` (agency): Name of the New York City agency (“agency” or “hiring agency”) where a job vacancy exists.
- `nyc_jobs_specifics.posting_type`: Identifies whether a job posting is an Internal or External posting. Internal postings are available to City employees only and external postings are available to the general public.
- `nyc_jobs_basics.job_category` : The occupational group in which the posted job belongs, such as: Administration & Human Resources; Communications & Intergovernmental Affairs; Constituent Services & Community Programs; Engineering, Architecture, & Planning; Finance, Accounting, & Procurement; Health; Technology, Data & Innovation; Legal Affairs; Building Operations & Maintenance; Policy, Research & Analysis; Public Safety, Inspections, & Enforcement; Social Services
- `nyc_jobs_basics.full_time_indicator`: "This denotes whether the job is a full time or part time employment; F - Full time; P - Part time"
- `nyc_jobs_basics.career_level`: "This denotes the career level of the job. The possible career levels are:; Student; Entry-level; Experienced (non-manager); Manager; Executive"
- `nyc_jobs_basics.salary_range_from`: The lowest salary on a job posting for a position within the salary band for the related civil service title.
- `nyc_jobs_basics.salary_range_to`: The highest salary on a job posting for a position within the salary band for the related civil service title.
- `nyc_jobs_basics.salary_frequency`: The frequency of proposed salary.  Possible salary frequency values include “hourly”, “daily”, and “annual”. 
- `nyc_jobs_basics.posting_date`: The date and time that a job vacancy was posted in MM/DD/YY format.

First, we load in additional libraries that will be used for the data exploration.

```{r additional library, message = FALSE}
library(plyr)
library(ggplot2)
```

We must then connect to our database and create dataframes in R that represent each table we are going to be working with.

```{r pull_data}
my_connection <- dbConnect(drv = odbc::odbc(),
                           Driver = "ODBC Driver 18 for SQL Server",
                           server = "tcp:data607project3server.database.windows.net,1433",
                           database = "Data 607 Project 3 Database",
                           uid = "Overview8909",
                           pwd = key_get("Project3SQL","Overview8909"),
                           encoding = "latin1"
                           )

nyc_jobs_basics_sql <- "select * from nyc_jobs_basics"
nyc_jobs_agencies_sql <- "select * from nyc_jobs_agencies"
nyc_jobs_codes_sql <- "select * from nyc_jobs_codes"
nyc_jobs_specifics_sql <- "select * from nyc_jobs_specifics"

nyc_jobs_basics <- dbGetQuery(my_connection, nyc_jobs_basics_sql)
nyc_jobs_agencies <- dbGetQuery(my_connection, nyc_jobs_agencies_sql)
nyc_jobs_codes <- dbGetQuery(my_connection, nyc_jobs_codes_sql)
nyc_jobs_specifics <- dbGetQuery(my_connection, nyc_jobs_specifics_sql)

dbDisconnect(my_connection)
```

To start, we'll first just get an overall count of the data.

```{r counts}
overall_count <- nyc_jobs_basics |> 
  dplyr::summarise(count = n())

sprintf("There are %i jobs currently posted.", overall_count$count)
```

As of Friday, March 17, there are 3,260 jobs currently available. 

How can we break this down further?

#### Agency Posted

What agencies are posting these jobs?

```{r agency}
agency_count <- nyc_jobs_basics |> 
  dplyr::summarise(count = n_distinct(work_unit))

sprintf("There are %i agencies.", agency_count$count)

counts_by_agency <- nyc_jobs_basics |> 
  group_by(work_unit) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

head(counts_by_agency, 10)
```

In total, there are 1,048 different agencies and no agency is more prevalent in job postings than another. The fact that the largest one only has 44 current job postings out of 3k+ (\~1%) shows that it's an extremely diverse collection of jobs.


#### Posting Type

Are a majority of these listings internal or external?

```{r posting_type}
counts_by_posting_type <- nyc_jobs_specifics |> 
  group_by(posting_type) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))
head(counts_by_posting_type)
```

Pretty close to 50/50, although internal is slightly higher at 55% vs. external's 45%.

#### Job Category

What job categories do these postings fall under?

```{r job_category}
category_count <- nyc_jobs_basics |> 
  dplyr::summarise(count = n_distinct(job_category))

sprintf("There are %i job categories", category_count$count)

counts_by_job_category <- nyc_jobs_basics |> 
  group_by(job_category) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

head(counts_by_job_category, 10)
```

In total, there are 192 distinct job categories. Of these, we can see that the most common categories are `Engineering, Architecture, & Planning` (\~13%), `Technology, Data, & Innovation` (\~8%), `Legal Affairs` (7%), and `Administration & Human Resources` (\~6%). 

Circling back to our goal of finding data science related jobs and skills, one way to hone in on this is to use category as a way to see what how many of these postings fall into categories with the words `analysis`, `analytics`, `data`, or `statistics`? 

```{r relevant_categories}
to_find <- c("Data", "Analysis", "Analytics", "Statistics")
matches <- unique(grep(paste(to_find, collapse="|"), nyc_jobs_basics$job_category, value=TRUE))

relevant_categories <- filter(nyc_jobs_basics, job_category %in% matches)

overall_relevant_count <- relevant_categories |> 
  dplyr::summarise(count = n())

counts_by_relevant_job_category <- relevant_categories |> 
  group_by(job_category) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(relevant_categories),
            percent_of_total = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

sprintf("There are %i jobs currently posted within a relevant category.", overall_relevant_count$count)

head(counts_by_relevant_job_category, 20)
```

Here, we can see there are 784 jobs currently posted with a category containing one of our keywords with `Policy, Research & Analysis` bubbling up as the second most popular relevant category at 85 postings (\~11% of this subset, \~3% of total).

#### Full/Part Time

What is the breakdown of these jobs by full vs. part time?

```{r full_part_time}
counts_by_full_part_time <- nyc_jobs_basics |> 
  group_by(full_time_indicator) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

head(counts_by_full_part_time)
```

It seems like there are a few rows where this isn't filled out, but a vast majority of the jobs posted are full-time listings.

#### Career Level

What is the breakdown of these jobs by career level?

```{r career_level}
counts_by_career_level <- nyc_jobs_basics |> 
  group_by(career_level) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

head(counts_by_career_level)
```

Interestingly, 72% (2.3k) of these jobs are for `Experienced (non-manager)` roles. 

#### Salary Information

How do these jobs look in terms of salaray?

##### Frequency

We can start with frequency -- how often are folks paid for these roles?

```{r salary_frequency}
counts_by_salary_frequency <- nyc_jobs_basics |> 
  group_by(salary_frequency) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

head(counts_by_salary_frequency)
```

A vast majority at \~89% (2.9k) are annual, although in order to properly compare compensation, we'll have to adjust the hourly and daily rates up to an annual value. We'll assume 40 hour weeks and 52 weeks in a year for the hourly folks and 365 days a year for the daily ones. I'll also round these adjusted salary ranges to the nearest $10,000.

```{r adjust_salary_frequency}
salary_from_adjusted <- c()
salary_to_adjusted <- c()

freq <- ""

for(i in 1:nrow(nyc_jobs_basics)) {
    freq <- nyc_jobs_basics$salary_frequency[i]
    if (freq == "Annual") {
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics$salary_range_from[i], 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics$salary_range_to[i], 10000))
    } else if (freq == "Hourly") {
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics$salary_range_from[i] * 40 * 52, 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics$salary_range_to[i] * 40 * 52, 10000))
    } else { # this means it's daily
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics$salary_range_from[i] * 365, 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics$salary_range_to[i] * 365, 10000))
    }
}

nyc_jobs_basics$salary_from_adjusted <- salary_from_adjusted
nyc_jobs_basics$salary_to_adjusted <- salary_to_adjusted

head(nyc_jobs_basics)
```

Now with this, how do the salary ranges look? Let's start with the beginning band (from).

```{r salary_from_chart}
count_by_salary_from <- nyc_jobs_basics |> 
  group_by(salary_from_adjusted, salary_frequency) |>
  dplyr::summarise(count = n(),
                   percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

salary_from_bar <-ggplot(data=count_by_salary_from
                   , aes(x=salary_from_adjusted
                         , y=count)) + 
  geom_bar(stat="identity", position=position_dodge()) + 
  scale_fill_brewer(palette="Paired")+
  theme_minimal()

salary_from_summary <- nyc_jobs_basics |>
  dplyr::summarise(mean = mean(salary_from_adjusted),
                   median = median(salary_from_adjusted),
                   min = min(salary_from_adjusted),
                   max = max(salary_from_adjusted))

salary_from_summary

salary_from_bar
```

The data looks to be right skewed here with a center around \~\$60k and a high of \~\$230k. The mean and median are close but not quite the same so it's not a perfectly normal distribution. What about the to (high) portion of the salary range?


```{r salary_to_chart}
count_by_salary_to <- nyc_jobs_basics |> 
  group_by(salary_to_adjusted, salary_frequency) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

salary_to_bar <-ggplot(data=count_by_salary_to
                   , aes(x=salary_to_adjusted
                         , y=count)) + 
  geom_bar(stat="identity", position=position_dodge()) + 
  scale_fill_brewer(palette="Paired")+
  theme_minimal()

salary_to_summary <- nyc_jobs_basics |>
  dplyr::summarise(mean = mean(salary_to_adjusted),
                   median = median(salary_to_adjusted),
                   min = min(salary_to_adjusted),
                   max = max(salary_to_adjusted))

salary_to_summary

salary_to_bar
```
This one looks less like a bell-curve (still right-skewed though) and seems to have a high frequency at \~\$60k as well, however there seems to be more jobs with rates at the tail ends of the spectrum. We can see that the max here goes up to \~\$250k with more activity in the \~\$100k+ range. The mean and median here have been brought up though at \$88k and \$80k respectively while the minimum is also at \$30k; in general, these rates pay more as they're the at the higher end of the spectrum.

What does salary look like if we assume most folks get the middle point of each posted range?

```{r salary_avg_chart}
nyc_jobs_basics$salary_mid_adjusted <- (nyc_jobs_basics$salary_to_adjusted + nyc_jobs_basics$salary_from_adjusted) / 2

count_by_salary_mid <- nyc_jobs_basics |> 
  group_by(salary_mid_adjusted, salary_frequency) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

salary_mid_bar <-ggplot(data=count_by_salary_mid
                   , aes(x=salary_mid_adjusted
                         , y=count)) + 
  geom_bar(stat="identity", position=position_dodge()) + 
  scale_fill_brewer(palette="Paired")+
  theme_minimal()

salary_mid_summary <- nyc_jobs_basics |>
  dplyr::summarise(mean = mean(salary_mid_adjusted),
                   median = median(salary_mid_adjusted),
                   min = min(salary_mid_adjusted),
                   max = max(salary_mid_adjusted))

salary_mid_summary

salary_mid_bar
```
Using the mid-point, we can see that the curve now peaks at \~\$65k. It's still a bit right-skewed with a tail trailing off at \~\$230k as well while the mean and median now are closer at ~\$\77k and \~\$75k respectively..

#### Post Date

In this data set, when were these jobs posted?

```{r post_date}
nyc_jobs_basics$posting_date <- as.Date(nyc_jobs_basics$posting_date, "%m/%d/%Y")

count_by_date <- nyc_jobs_basics |> 
  group_by(year = format(posting_date, '%Y')) |>
  dplyr::summarise(count = n(),
            percent = 100 * n()/nrow(nyc_jobs_basics),
            .groups = 'drop') |>
  arrange(desc(count))

head(count_by_date, 10)
```

It looks like a majority of these job listings are from this year and last year, however some of them are even from 2014. For the most part though, it seems like they're mostly recent as 48% of them are from this year, but it's only been \~3.5 months as it's only March.

## Data Analysis

Now that the data has been loaded into SQL, tidied, and explored, we will answer the project's primary question: "What are the most valued data science skills?" We will accomplish this by searching job postings on the City of New York official jobs site, identifying and quantifying requested skills in job descriptions which are part of the data science workflow. Our group will theorize about which skills in the data science workflow are "most valued" by determining how many jobs in the NYC government request those skills.

##### Querying Data

As we only need a subset of data from two tables for our analysis, we will go ahead and run a SQL query to retrieve the columns that we want.

Not every field within our database is helpful for answering the project question. Here we create a custom table through a join, pulling only fields which can be searched for data science skills: business_title (e.g. data analyst), job_category (e.g. Finance), job_description, minimum_qual (e.g. college major), and preferred_skills. This table is 3,260 rows long with each row being a job posting. 


```{r get combined tables}

my_connection <- dbConnect(drv = odbc::odbc(),
                           Driver = "ODBC Driver 18 for SQL Server",
                           server = "tcp:data607project3server.database.windows.net,1433",
                           database = "Data 607 Project 3 Database",
                           uid = "Overview8909",
                           pwd = key_get("Project3SQL","Overview8909"),
                           encoding = "latin1"
                           )

nyc_jobs_combination_sql <- r"(
SELECT b.job_id, b.business_title, b.job_category, s.job_description, s.minimum_qual, s.preferred_skills
FROM nyc_jobs_basics as b 
JOIN nyc_jobs_specifics as s ON b.job_id = s.job_id
)"

nyc_jobs_combination <- dbGetQuery(my_connection, nyc_jobs_combination_sql)

dbDisconnect(my_connection)

nyc_jobs_combination[,4:6] <- lapply(nyc_jobs_combination[,4:6], tolower)

head(nyc_jobs_combination)
```

#### Approach

For each job posting (row) we want to find all mentions of data science skills, searching them and compiling a count into a new dataframe. 

First we need to define the skills we'll search for. These keywords were generated using a combination of domain knowledge, Google Ads keyword planner, and skimming the jobs data set for requested skills. While leveraging regular expressions could make this list of keywords shorter, because there are generally so many keywords of interest, the amount of time to write regex for each would be burdensome. Therefore for this project we will searchf for whole keywords, while avoiding ambiguity.

```{r keyword definition}
data_acq_keys <- tolower(
  c("SQL", "MySQL", "data acquisition", "database", "dbms", "JSON", "API", "data entry", "data scraping", "mongodb", "postgre", "mariadb", "data extraction", "web scraping", "etl", "data collection", "web crawling", "data warehouse", "azure", "Amazon Web Services" )
)
data_prep_keys <- tolower(
  c("Alteryx", "Knime",  "data engineering", "preprocessing", "data preparation", "data cleaning", "data wrangling", "data transformation", "data integration", "normalization", "imputation", "data formatting", "data merging", "data enrichment", "data augmentation", "data sampling","data reduction", "dplyr", "stringr", "pandas" )
)
data_exp_keys <- tolower(
  c("exploratory data analysis", "feature selection", "data research", "data exploration", "Descriptive Statistics", "Data Profiling", "Data Quality Assessment", "Data Sampling", "Dimensionality Reduction", "dimension reduction", "Feature Engineering", "Correlation Analysis", "Outlier Detection",  "databricks","data quality analysis","data summary", "data explorer", "exploratory data analysis", "kusto", "azure data explorer")
)
data_anal_keys <- tolower(
  c("data modeling", "machine learning", "data analysis", "artificial intelligence", "quantitative analysis", "predictive analysis", "regression", "Statistical Analysis", "statistical analytics", "Cluster Analysis", "Hypothesis Testing", "A/B Testing", "Data Mining", "natural language processing", "scikit-learn", "pytorch", "data analytics","pandas", "RStudio", "spss" )
)
data_vis_keys <- tolower(
  c("Tableau", "PowerBI", "quantitative findings", "creating graphs", "chart creation", "data reporting", "data visualization", "Data Storytelling", "Data Presentation", "Information Design", "Dashboard Creation", "Infographics", "network visualization", "user experience design", "datawrapper", "apache superset", "scientific visualization", "visual analytics", "visual encodings", "data animation")
)

data_science_categories <- list(
  data_acquisition = data_acq_keys, 
  data_preparation = data_prep_keys,
  data_exploration = data_exp_keys, 
  data_analysis = data_anal_keys, 
  data_visualization = data_vis_keys
)
```

#### Skill Scrape

To begin this analysis we can look at the count of keywords within the relevant dataframe columns as a whole. Although, this new dataframe can not be used for much further analyis, it would provide us a simple answer to which data science skills are most in demand based on this dataset.

```{r detect keywords, message=FALSE}
library(scales)
library(stringi)

search_keywords <- function(df, keywords, category) {
  # Create a vector to store the counts of each keyword
  word_count <- rep(0, length(keywords))
  
  # Loop through each row of the dataframe
  for (i in 1:nrow(df)) {
    # Convert all the text in each column to lowercase and combine them into a single string
    text <- paste(df[i, ], collapse = " ")
    # Loop through each keyword in the keyword list
    for (j in 1:length(keywords)) {
      # If the keyword is in the text, increment the count for that keyword
        # ignore.case to make sure either "Python" or "python" is counted
      if (stri_detect_regex(text, paste0("\\b", keywords[j], "\\b"))) {
        word_count[j] <- word_count[j] + 1
      }
    }
  }
  
  # Return the word counts as a tibble
  keywords_count <- tibble(
    category = category,
    keyword = keywords,
    count = word_count
    )
  return(keywords_count)
}

keyword_count <- rbind(
  search_keywords(nyc_jobs_combination, data_acq_keys, "Data Acquisition"),
  search_keywords(nyc_jobs_combination, data_prep_keys, "Data Preparation"),
  search_keywords(nyc_jobs_combination, data_exp_keys, "Data Exploration"),
  search_keywords(nyc_jobs_combination, data_anal_keys, "Data Analysis"),
  search_keywords(nyc_jobs_combination, data_vis_keys, "Data Visualization")
)
```

The dataframe keyword_count now shows the volume of mentions for each skill inside the NYC jobs dataset.

```{r head keyword_count}
head(arrange(keyword_count,desc(count)), n = 10)
```
##### Mutating Data Based on Keywords

In order to be able to truly relate the keywords within each column to other information within the dataset, we will need to create new columns on the dataset that detects matches for each data science skill category. The number of matches is stored as an integer for each column. We then also create a total matches that can perhaps be used to measure how relevant a job is to data science.

```{r keyword mutation, warning=FALSE}
# Create the new columns in the data frame which count up keyword matches within each category
for (category_name in names(data_science_categories)) {
  nyc_jobs_combination[[category_name]] <- as_tibble(sapply(data_science_categories[[category_name]],grepl,apply(nyc_jobs_combination[, 4:6], 1, paste, collapse = " "))) %>%
    dplyr::mutate(!!category_name := as.integer(rowSums(across(where(is.logical)))), .keep="none") %>%
    pull(!!category_name)
}

# Create a column for total data science matches in the data frame
nyc_jobs_combination <- nyc_jobs_combination |>
  dplyr::mutate(total_datascience_keywords = as.integer(rowSums(across(data_acquisition:data_visualization))))

glimpse(nyc_jobs_combination)
```

As we'll be utilizing this dataframe between group members, it makes sense to make a new database table for storing this dataframe data so it can quickly be referenced.

```{r new combination table, eval = FALSE}

query5 <- r"(
  CREATE TABLE nyc_jobs_combination(
    job_id INT,
    business_title VARCHAR(150),
    job_category VARCHAR(300),
    job_description VARCHAR(MAX),
    minimum_qual VARCHAR(MAX),
    preferred_skills VARCHAR(4000),
    data_acquisition INT,
    data_preparation INT,
    data_exploration INT,
    data_analysis INT,
    data_visualization INT,
    total_datascience_keywords INT,
    PRIMARY KEY (job_id));
)"

my_connection <- dbConnect(drv = odbc::odbc(),
                           Driver = "ODBC Driver 18 for SQL Server",
                           server = "tcp:data607project3server.database.windows.net,1433",
                           database = "Data 607 Project 3 Database",
                           uid = "Overview8909",
                           pwd = key_get("Project3SQL","Overview8909"),
                           encoding = "latin1"
                           )

query_combination <- c(query5)
for (query in query_combination) {
  dbExecute(my_connection,query)
  Sys.sleep(1)
}

dbAppendTable(my_connection,"nyc_jobs_combination", nyc_jobs_combination)

dbDisconnect(my_connection)
```

With the new table created and populated, we can access it again at anytime by running the following:

```{r get combination table}
nyc_jobs_combination_sql <- "select * from nyc_jobs_combination"

my_connection <- dbConnect(drv = odbc::odbc(),
                           Driver = "ODBC Driver 18 for SQL Server",
                           server = "tcp:data607project3server.database.windows.net,1433",
                           database = "Data 607 Project 3 Database",
                           uid = "Overview8909",
                           pwd = key_get("Project3SQL","Overview8909"),
                           encoding = "latin1"
                           )

nyc_jobs_combination <- dbGetQuery(my_connection, nyc_jobs_combination_sql)

dbDisconnect(my_connection)
```

We can also breakdown the data by category and see data acquisition skills are most in demand within our data.
```{r high level category count}

keyword_count |>
  group_by(category) |>
  dplyr::summarise(count = sum(count)) |>
  arrange(count) |>
  ggplot(aes(x= reorder(category,count),y=count, fill = category))+
    geom_bar(stat="identity", show.legend = FALSE) +
    xlab("Data Science Category") +
    ylab("Job Listing Count") +
    labs(title = "Data Science Categories in NY Job Listings", caption = "(Based on data from NYC OpenData)") +
    geom_text(aes(label=count)) +
    coord_flip()

```

There were 47 individual data science skills mentioned on the NYC jobs postings.

```{r keyword_count count}
keyword_count |>
  dplyr::filter(count>0) |>
  nrow()
```

Of the 3,261 jobs listed on the NYC government site, 57.5% require a data science skill.

```{r percentage skills}
nyc_jobs_combination |>
  dplyr::filter(total_datascience_keywords>0) |>
  nrow()/nrow(nyc_jobs_combination)
```

Showing only skills with more than 10 occurrences in the dataset, then charting them. Colors based on category of Data Science skill.

To answer the primary project question, the most in-demand data science skills were "database" (459 mentions) followed by "data entry" (261), "data analysis" (214), "SQL" (193), and "data collection" (117). Four of these top five fall under the category of "Data Acquisition."

```{r keyword occurence bar graph, message = FALSE}
keyword_count |>
  dplyr::filter(count>=10) |>
  dplyr::group_by(keyword,category) |>
  dplyr::summarise(count = sum(count)) |>
  arrange(count) |>
  ggplot(aes(x= reorder(keyword,count),y=count, fill = category))+
    geom_bar(stat="identity", show.legend = TRUE) +
    xlab("Data Science Skill") +
    ylab("Job Listing Count") +
    labs(title = "Data Science Skills in NYC Job Listings", caption = "(Based on data from NYC OpenData)") +
    guides(fill=guide_legend(title="Skill Category")) +
    geom_text(aes(label=count)) +
    coord_flip()
```


While 57.5% of all job posting required a data science skill, those jobs requested a varied array of skills. The three most in-demand skills, "database," "data entry," and "data analysis," were found in 14.1%, 8.0%, and 6.6% of all job postings. 

```{r percentage skills breakdown}
keyword_count <- keyword_count |> 
  group_by(category,keyword) |>
  mutate(pct_all_jobs = percent(count/nrow(nyc_jobs_combination)))
```

```{r more keyword breakdown, message = FALSE}
keyword_count |>
  filter(pct_all_jobs>=0.5) |>
  group_by(keyword,category,pct_all_jobs) |>
  dplyr::summarise(pct = sum(count)/nrow(nyc_jobs_combination))|>
  ggplot(aes(x= reorder(keyword,pct),y=round(pct,2)*100, fill = category))+
    geom_bar(stat="identity", show.legend = TRUE) +
    xlab("Data Science Skill") +
    ylab("% of All Jobs Requiring Skill") +
    labs(title = "Top Data Science Skills as % of NYC Job Listings", caption = "(Based on data from NYC OpenData)") +
    guides(fill=guide_legend(title="Skill Category")) +
    geom_text(aes(label=percent(round(pct,3)))) +
    coord_flip()
```

##### Orginal Dataframe Keyword Integration

If we combine our scraped skills list with information in the original database, we can look at the median base salaries for jobs requiring a given skill. Median base salaries are based on the median value of aggregated "Salary From" value for jobs requiring a given skill.

```{r dataframe keyword integration}
ds_keywords_full <- c(data_acq_keys,data_prep_keys,data_exp_keys,data_anal_keys,data_vis_keys)

nyc_jobs_w_skills <- nyc_jobs_combination |> 
  rowwise() |>
  dplyr::mutate(skill_extract = paste(ds_keywords_full[map_lgl(ds_keywords_full, ~ any(grepl(.x, c_across(business_title:preferred_skills))))], collapse = ", "))

nyc_jobs_skills <- nyc_jobs_w_skills[c('job_id','skill_extract')]
```

```{r dataframe keyword integration 2}
library(plyr)

my_connection <- dbConnect(drv = odbc::odbc(),
                           Driver = "ODBC Driver 18 for SQL Server",
                           server = "tcp:data607project3server.database.windows.net,1433",
                           database = "Data 607 Project 3 Database",
                           uid = "Overview8909",
                           pwd = key_get("Project3SQL","Overview8909"),
                           encoding = "latin1"
                           )

nyc_jobs_salaries <- dbGetQuery(my_connection, "select job_id, salary_range_from, salary_range_to, salary_frequency from nyc_jobs_basics")

dbDisconnect(my_connection)

salary_from_adjusted <- c()
salary_to_adjusted <- c()
freq <- ""

for(i in 1:nrow(nyc_jobs_salaries)) {
    freq <- nyc_jobs_salaries$salary_frequency[i]
    if (freq == "Annual") {
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics$salary_range_from[i], 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics$salary_range_to[i], 10000))
    } else if (freq == "Hourly") {
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics$salary_range_from[i] * 40 * 52, 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics$salary_range_to[i] * 40 * 52, 10000))
    } else { # this means it's daily
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics$salary_range_from[i] * 365, 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics$salary_range_to[i] * 365, 10000))
    }
}
nyc_jobs_salaries$salary_from_adjusted <- salary_from_adjusted
nyc_jobs_salaries$salary_to_adjusted <- salary_to_adjusted
```

```{r dataframe keyword integration 3, warning = FALSE, message = FALSE}
skills_salary <- merge(nyc_jobs_skills,nyc_jobs_salaries)
skills_salary <- skills_salary[c('salary_from_adjusted','skill_extract')]

library(splitstackshape)
split_skills <- cSplit(skills_salary, "skill_extract", ", ")

skills_long <- split_skills |>
  pivot_longer(cols=c('skill_extract_01',
                      'skill_extract_02',
                      'skill_extract_03',
                      'skill_extract_04',
                      'skill_extract_05',
                      'skill_extract_06',
                      'skill_extract_07',
                      'skill_extract_08',
                      'skill_extract_09',
                      'skill_extract_10',
                      'skill_extract_11'),
               names_to='skill_extract',
               values_to='skill')

skills_long <- skills_long[c('salary_from_adjusted','skill')]
skills_long <- skills_long[complete.cases(skills_long),]
```

skills_long dataframe has the information we need to calculate median salary by skill. Now we'll just add the data science skills categorizations from our keyword_count df above, and remove hourly wages. 

```{r median salary by skill, message = FALSE}
skills_final <- merge(x=skills_long,y=keyword_count,by.x=c('skill')
                         ,by.y=c('keyword'))

skills_final <- skills_final[c('skill','salary_from_adjusted','category')]
skills_final <- skills_final %>%
  dplyr::filter(salary_from_adjusted>0) %>%
  dplyr::group_by(skill,category) %>%
  dplyr::summarise(median_starting_salary = median(salary_from_adjusted))
  
```

Graphing all 47 skills mentioned in the job postings would make an overly crowded chart. Therefore we will a) Make a combined chart showing the top 10 skills across categories by salary; B) Display all skills by first grouping them by skill category before charting. Even after grouping, a cowplot is not big enough to make the skill labels read-able, therefore we will print them individually. The knitted R Pubs document will place them in one container for easier comparison.

```{r}
top_salary_skills <- skills_final |> filter (median_starting_salary > 80000) |> dplyr::arrange(median_starting_salary) |> ggplot(aes(x= reorder(skill,median_starting_salary),y=median_starting_salary, fill = category)) + geom_bar(stat="identity", show.legend = TRUE) + xlab("Data Science Skill") + ylab("Median Starting Salary of Job Requiring Skill") + labs(title = "Top Skills by Base Salary in NYC Job Listings", caption = "(Based on data from NYC OpenData)") + guides(fill=guide_legend(title="Skill Category")) + coord_flip() + scale_y_continuous(labels = scales::label_dollar(scale=0.001,suffix="K")) + geom_text(aes(label=dollar_format(scale=0.001,suffix="K")(median_starting_salary)))

skills_acq_graph <- skills_final |> filter(category == 'Data Acquisition') |> dplyr::arrange(median_starting_salary) |> ggplot(aes(x= reorder(skill,median_starting_salary),y=median_starting_salary, fill = category)) + geom_bar(stat="identity", show.legend = TRUE,fill='#f8766d') + xlab("Data Science Skill") + ylab("Median Starting Salary of Job Requiring Skill") + labs(title = "Data Acquisition Skills by Base Salary in NYC Job Listings", caption = "(Based on data from NYC OpenData)") + guides(fill=guide_legend(title="Skill Category")) + coord_flip() + scale_y_continuous(labels = scales::label_dollar(scale=0.001,suffix="K")) + geom_text(aes(label=dollar_format(scale=0.001,suffix="K")(median_starting_salary)))

skills_analysis_graph <- skills_final |> filter(category == 'Data Analysis') |> dplyr::arrange(median_starting_salary) |> ggplot(aes(x= reorder(skill,median_starting_salary),y=median_starting_salary, fill = category)) + geom_bar(stat="identity", show.legend = TRUE,fill='#a3a500') + xlab("Data Science Skill") + ylab("Median Starting Salary of Job Requiring Skill") + labs(title = "Data Analysis Skills by Base Salary in NYC Job Listings", caption = "(Based on data from NYC OpenData)") + guides(fill=guide_legend(title="Skill Category")) + coord_flip() + scale_y_continuous(labels = scales::label_dollar(scale=0.001,suffix="K")) + geom_text(aes(label=dollar_format(scale=0.001,suffix="K")(median_starting_salary)))

skills_exploration_graph <- skills_final |> filter(category == 'Data Exploration') |> dplyr::arrange(median_starting_salary) |> ggplot(aes(x= reorder(skill,median_starting_salary),y=median_starting_salary, fill = category)) + geom_bar(stat="identity", show.legend = TRUE,fill='#00bf7d') + xlab("Data Science Skill") + ylab("Median Starting Salary of Job Requiring Skill") + labs(title = "Data Exploration Skills by Base Salary in NYC Job Listings", caption = "(Based on data from NYC OpenData)") + guides(fill=guide_legend(title="Skill Category")) + coord_flip() + scale_y_continuous(labels = scales::label_dollar(scale=0.001,suffix="K")) + geom_text(aes(label=dollar_format(scale=0.001,suffix="K")(median_starting_salary)))

skills_preparation_graph <- skills_final |> filter(category == 'Data Preparation') |> dplyr::arrange(median_starting_salary) |> ggplot(aes(x= reorder(skill,median_starting_salary),y=median_starting_salary, fill = category)) + geom_bar(stat="identity", show.legend = TRUE,fill='#00b0f6') + xlab("Data Science Skill") + ylab("Median Starting Salary of Job Requiring Skill") + labs(title = "Data Preparation Skills by Base Salary in NYC Job Listings", caption = "(Based on data from NYC OpenData)") + guides(fill=guide_legend(title="Skill Category")) + coord_flip() + scale_y_continuous(labels = scales::label_dollar(scale=0.001,suffix="K")) + geom_text(aes(label=dollar_format(scale=0.001,suffix="K")(median_starting_salary)))

skills_visualization_graph <- skills_final |> filter(category == 'Data Visualization') |> dplyr::arrange(median_starting_salary) |> ggplot(aes(x= reorder(skill,median_starting_salary),y=median_starting_salary, fill = category)) + geom_bar(stat="identity", show.legend = TRUE,fill='#e76bf3') + xlab("Data Science Skill") + ylab("Median Starting Salary of Job Requiring Skill") + labs(title = "Data Visualization Skills by Base Salary in NYC Job Listings", caption = "(Based on data from NYC OpenData)") + guides(fill=guide_legend(title="Skill Category")) + coord_flip() + scale_y_continuous(labels = scales::label_dollar(scale=0.001,suffix="K")) + geom_text(aes(label=dollar_format(scale=0.001,suffix="K")(median_starting_salary)))
```

There are 12 skills which offer a median starting salary of $85,000 or more in the NYC job listings. Perhaps surprisingly, only one "Data Visualization" skill is in the top 12, tied for #1: "UX design." In general though, a mix of skill categories make up the top 12. Top paying skills by category: 
- Data Acquisition: "JSON" - $100K
- Data Analysis: "Data Modeling" - $95K
- Data Exploration: "Data Profiling" - $100K
- Data preparation: "Data Formatting" - $95K
- Data Visualization: "User Experience Design" - $100K

If one was interested in pursuing an NYC government job, and was particularly interested in one of these categories, they could use these data as a reference point on where to invest their time. 

```{r}
#Printing charts
top_salary_skills
skills_acq_graph
skills_analysis_graph
skills_exploration_graph
skills_preparation_graph
skills_visualization_graph
```

### Analyzing Association With Other Factors

Since we've answered the question about which are the most valued data science skills, let's also take a stab at related questions with the information we have.

##### Which job category has the most postings for Data related roles?

Let's assume we were a data scientists looking through these postings, and we wanted to filter down the jobs to be most relevant to data science through the job category. As there is no direct "data science" category, we can analyze our data to find which category has the most postings for data related roles.

First, we'll need to define a data related role. While skimming through the data, in this format we can notice that there are many jobs where one or two keywords are found but they aren't really focused on the data aspects. In order to get around this we'll define a data related role as a job where there are three or more total keywords detected.

```{r job category breakdown}
library(RColorBrewer)

nyc_jobs_combination %>%
  filter(total_datascience_keywords > 2) %>%
  group_by(job_category) %>%
  dplyr::summarise(count = n())|>
  arrange(desc(count)) |>
  head(10) |>
  ggplot(aes(x= reorder(job_category,count),y=count, fill = job_category))+
    geom_bar(stat="identity", show.legend = FALSE) +
    xlab("Job Category") +
    ylab("Job Listing Count") +
    labs(title = "Data Roles in Job Category Count", caption = "(Based on data from NYC OpenData)") +
    geom_text(aes(label=count)) +
    scale_fill_brewer(palette = 'RdYlGn') +
    coord_flip()
```
Looking into the result we have here, it comes as no surprise that the "Technology, Data & Innovation" job category has the most data roles by over 300% of the next one "Engineering, Architecture & Planning" which is also not much of a surprise at second place. It's more interesting in this case to look at job categories that are unexpected results. 

"Policy, Research & Analysis" while having analysis in the name does not immediately make one think of data science roles. Policy evokes more of a feel of a senator going through old documents to outline what exactly should be legal or not. However in this day and age, data skills are necessary in this field to gather, organize, and analyze data in order to identify patterns, trends, and insights that can inform policy decisions and drive evidence-based research. A much better approach than just feeling out what would make a good policy.

"Constituent Services & Community Programs" is also another one out of the left field. However, the reasoning becomes more clear thinking about it in this role, you may need to analyze data related to the community's demographics, needs, and priorities to develop effective programs and initiatives that meet their needs. This could involve analyzing survey data, census data, and other relevant data sources to identify patterns and trends.

##### Which NYC agency has the most postings for Data related roles?

Let's say we're doing an investigation on what agency in NYC might be the easiest one to apply to in order to get a role related to data science. We can determine this by joining agency information from other tables on our combination dataframe. Then we take the same filtering criteria and group by agency.

```{r job agency breakdown}
nyc_jobs_basics_small <- nyc_jobs_basics |>
  select(job_id,work_unit)

nyc_jobs_combination |>
  filter(total_datascience_keywords > 2) |>
  inner_join(nyc_jobs_basics_small, by="job_id") |>
  inner_join(nyc_jobs_agencies, by = "work_unit")|>
  group_by(agency) |>
  dplyr::summarise(count = n()) |>
  arrange(desc(count)) |>
  head(10) |>
  ggplot(aes(x= reorder(agency,count),y=count, fill = agency))+
    geom_bar(stat="identity", show.legend = FALSE) +
    xlab("Job Agency") +
    ylab("Job Listing Count") +
    labs(title = "Data Roles in Job Agency Count", caption = "(Based on data from NYC OpenData)") +
    geom_text(aes(label=count)) +
    scale_fill_brewer(palette = 'RdYlGn') +
    coord_flip()
```
As we can see, the distribution of data science job by agency for the top 10 is more spread out than the distribution per category. This can lead us to believe that the job agency matters less for job listings being determined as data science jobs. We can also determine that if we were to apply for jobs by only looking at the agency, picking a job from the department of environmental protection would be our best bet.

##### Which are the most common job titles related to data skills?

Now let's say that we wanted to look at what job titles we should be applying for based on what type of data science skill we've specialize in. Using our data we can determine what job titles correspond to each skillset the most. We use the civil service title for determining job titles as they are much more general than the business titles.

```{r job title breakdown, fig.width=20, fig.height=20, message=FALSE}
library(patchwork)
library(purrr)
library(cowplot)
library(glue)

nyc_jobs_basics_small2 <- nyc_jobs_basics |>
  select(job_id,title_code_num)

nyc_jobs_combination2 <- nyc_jobs_combination |>
  inner_join(nyc_jobs_basics_small2, by="job_id") |>
  inner_join(nyc_jobs_codes, by="title_code_num") |>
  group_by(civil_service_title) |>
  dplyr::summarise(total_datascience_keywords = sum(total_datascience_keywords), data_acquisition = sum(data_acquisition), data_analysis = sum(data_analysis), data_visualization = sum(data_visualization), data_preparation = sum(data_preparation), data_exploration = sum(data_exploration)) |>
  arrange(desc(total_datascience_keywords))

# create a vector of keyword column names
keyword_cols <- c("total_datascience_keywords", "data_acquisition", "data_analysis", "data_visualization", "data_preparation", "data_exploration")

# create an empty list to store plots
plot_list <- list()

for (col in keyword_cols) {
  p <- nyc_jobs_combination2 |>
    slice_max(!!sym(col),n = 8,with_ties = FALSE) |>
    ggplot(aes(x = reorder(civil_service_title,!!sym(col)), y = !!sym(col), fill = civil_service_title)) +
      geom_bar(stat = "identity", show.legend = FALSE) +
      ggtitle(col) +
      xlab("Job Title") +
      ylab("Keyword Count") +
      labs(title = str_replace_all(glue("Job Titles with {col}"),"_"," "), caption = "(Based on data from NYC OpenData)") +
      geom_text(aes(label=!!sym(col))) +
      scale_fill_brewer(palette = 'Accent') +
      coord_flip()
      # add the plot to the plot_list
  plot_list[[col]] <- p
}

plot_grid(plot_list[[2]], plot_list[[3]],
          plot_list[[4]], plot_list[[5]],
          plot_list[[6]], plot_list[[1]],
          nrow = 3, ncol = 2)
```

We can answer quite a few related questions with these graphs. First and foremost, we can tackle the jobs that require the most data science skills as a whole. Those jobs would include: a city research scientist, computer specialist, and community coordinator. We'll see city research scientist and computer specialist pop up in the top 3 of the different graphs again and again, however community coordinator has a few skills from each of the sector but that still adds up.

Those who are interested in specifically data acquisition could focus on being a a city research scientist, computer specialist, or community coordinator.

Those who are interested in specifically data visualization do not have many choices, and could focus on being a a city research scientist, IT project specialist, or eligibility specialist.

Those who are interested in specifically data preparation could focus on being a a city research scientist, computer specialist, or computer systems manager.

Those who are interested in specifically data analysis could focus on being a a city research scientist, administrative staff analyst, or staff analyst.

Those who are interested in specifically data exploration could focus on being a a city research scientist, clerical associate, or agency attorney.

##### Do jobs which require data skills pay more than those which don't? 

Say we're interested in money (to be fair who isn't?) Specifically the amount of money that data science skills bring in. We want to see if there is a positive correlation between the mean salary and the total amount of data skills that were detected in the job posting. To do this we adjust the salary to a normalized yearly salary like we did in the data exploration. Then we plot a scatterplot with a correlation line through to determine any relationship. 

```{r job money skill breakdown}
nyc_jobs_basics_small3 <- nyc_jobs_basics |>
  select(job_id,salary_range_from, salary_range_to, salary_frequency)

salary_from_adjusted <- c()
salary_to_adjusted <- c()

freq <- ""

for(i in 1:nrow(nyc_jobs_basics_small3)) {
    freq <- nyc_jobs_basics_small3$salary_frequency[i]
    if (freq == "Annual") {
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics_small3$salary_range_from[i], 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics_small3$salary_range_to[i], 10000))
    } else if (freq == "Hourly") {
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics_small3$salary_range_from[i] * 40 * 52, 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics_small3$salary_range_to[i] * 40 * 52, 10000))
    } else { # this means it's daily
      salary_from_adjusted <- append(salary_from_adjusted, round_any(nyc_jobs_basics_small3$salary_range_from[i] * 365, 10000))
      salary_to_adjusted <- append(salary_to_adjusted, round_any(nyc_jobs_basics_small3$salary_range_to[i] * 365, 10000))
    }
}

nyc_jobs_basics_small3$salary_from_adjusted <- salary_from_adjusted
nyc_jobs_basics_small3$salary_to_adjusted <- salary_to_adjusted

nyc_jobs_basics_small3 <- nyc_jobs_basics_small3 |>
  select(job_id,salary_from_adjusted, salary_to_adjusted) |>
  dplyr::mutate(salary_mid_adjusted = (salary_from_adjusted + salary_to_adjusted)/2) |>
  select(job_id,salary_mid_adjusted)

nyc_jobs_combination3 <- nyc_jobs_combination |>
  inner_join(nyc_jobs_basics_small3, by="job_id")

ggplot(nyc_jobs_combination3,aes(x=total_datascience_keywords,y=salary_mid_adjusted)) +
    geom_jitter() +
    geom_smooth(method = lm, formula = y ~ x) +
    labs(title = "Correlation Between # of Data Skills Required and Pay", caption = "(Based on data from NYC OpenData)") +
    xlab("Data Science Keyword Count") +
    ylab("Salary Range Median")
```
The scatterplot we end up with is very dense and concentrated on those jobs without any data science keywords in the listing. However, those without any data science keywords in the listing are also grouped up towards the bottom quadrant of pay. As we increase the data science keyword count, the lower end of the salary range increases. This does provide a positive relationship with data science keyword count and median salary range of a job listing. However, the lack of data towards the higher end of the keyword counts doesn't instill confidence in this assessment. 

The next step is finding out, just how correlated the two variables are.

```{r correlation skills and pay}

cor(x=nyc_jobs_combination3$total_datascience_keywords,y=nyc_jobs_combination3$salary_mid_adjusted)

```
With a correlation of 0.11 between pay and total data science keyword count, we can say that within our data there is no relationship between the two. A very unfortunate discovery.

## Conclusions

#### Data Cleaning Conclusions
- The Department of Citywide Administrative Services compiled our data science skills data.
- The data is hosted by NYC OpenData and includes detailed info for all current openings for New York City government-related positions. 
- The data source was originally published in 2013 and updates weekly.
- We downloaded the data, cleaned it, and divided its contents into a four-table information schema:
1. NYC Jobs Basics (business title, salary, posting date, etc.)
2. NYC Jobs Specifics (minimum qualifications, work location, job description, etc.)
3. NYC Jobs Agencies (work agency and agency location)
2. NYC Jobs Codes (civil service title, title classification, and title level)
- Cleaning the data primarily entailed removing non alpha-numeric characters, changing empty strings to NA, and rounding long numeric figures such as salaries.
- After cleaning the data, we pushed the schema to an Azure, cloud-based database, which we access as a group via SQL in our RMD files.
- For the analysis portion, we created a dictionary of data science skills, grouped by data science workflow category (acquisition, analysis, exploration, preparation, and visualization). We searched each job posting for these skills, tallying which skills were mentioned most-often, and using that information to answer the project question "Which are the most valued data science skills?"

#### Data Exploration Conclusions
- The data included 3,260 job listings, 94% of which were posted in 2022 or 2023, but with some dating back to 2014. 
- 92% of jobs were full-time, 55% were for the NYC government itself (vs. contractor positions), and 89% received an annual salary (vs. hourly or daily).
- 1048 agencies and 192 job categories were represented in the jobs data. The Bureau of Wastewater Treatment (BWT) was the top agency, with 43 postings or 1.32% of the total. "Engineering, Architecture, and Planning" was the top job category, with 433 positions or 13.28% of the total.
- Including estimates for hourly and daily workers, the median salary was $75,000 per year, while the mean was $76,716. The minimum was $30,000 and the maximum was $230,000. The salaries adhere to a generally normal distribution. 

#### Most Valued Skills Conclusions
- Of the 3,261 jobs listed on the NYC government site, more than half (57.5%) required a data science skill. 
- There were 47 different data science skills mentioned in the listings. The most in-demand skill, "database," was found in 459 or 14.1% of all posts, followed by "data entry" in 214 posts (8%), "data analysis" in 117 posts (6.6%), "SQL" (193 posts - 5.9%), and "data collection" (117 posts - 3.6%)
- The top 12 skills by base salary came from five different data science workflow categories. The top 12 included: four data preparation skills, four data acquisition skills, two data analysis skills, and 1 skill each from data exploration and data visualization.
- Job postings including the skills "UX Design," "JSON," and "Data Profiling," had the highest median base salaries, all at $100K. 

#### Findings Related to "Most Valued Skills" Conclusions
- If you're interested in pursuing a data science job, many different NYC government agencies offer opportunities. 51 different agencies have a job posting requiring DS skills, most notably The Department of Environmental Protection (42 postings), The Department of Social Services (36), and Housing Preservation and Development (35).
- Don't limit your career search to jobs classified under the "Technology, Data, & Innovation" category. While that categorization accounts for the most DS jobs (113), there are many in "qualitative" areas: for example 21 in Constituent Services & Community Programs and 15 in Administration & Human Resources.
- We found a weak but visible correlation between the number of data science skills in a job posting and the median salary range for that job. 

...