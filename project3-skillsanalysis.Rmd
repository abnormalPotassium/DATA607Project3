
## Analysis of Job Descriptions

Now that the data has been loaded into SQL, tidied, and explored, we will answer the project's primary question: "What are the most valued data science skills?" We will accomplish this by searching job postings on the City of New York official jobs site, identifying and quantifying requested skills in job descriptions which are part of the data science workflow. Our group will theorize about which skills in the data science workflow are "most valued" by determining how many jobs in the NYC government request those skills.

#### Connecting RMD to Database 

First, connecting to our group's database and loading helpful packages. 

```{r connect to Azure SQL Server}
library(tidyverse)
library(odbc)
library(DBI)
library(keyring)

if (!("Overview8909" %in% as_vector(key_list()[2]))) {
  key_set("Project3SQL","Overview8909")
}

my_connection <- dbConnect(drv = odbc::odbc(),
                           Driver = "ODBC Driver 18 for SQL Server",
                           server = "tcp:data607project3server.database.windows.net,1433",
                           database = "Data 607 Project 3 Database",
                           uid = "Overview8909",
                           pwd='Caterer Uproot Streak Herself Greeter Coma Eatable8 Mystified',
                           encoding = "latin1"
                           )
```


#### Pulling Data from Database

Not every field within our database is helpful for answering the project question. Here we create a custom table through a join, pulling only fields which can be searched for data science skills: business_title (e.g. data analyst), job_category (e.g. Finance), job_description, minimum_qual (e.g. college major), and preferred_skills. This table is 3,260 rows long with each row being a job posting. Making Description and p

```{r}
nyc_jobs_combination_sql <- "SELECT b.job_id, b.business_title, s.job_category, s.job_description, s.minimum_qual, s.preferred_skills FROM nyc_jobs_basics as b JOIN nyc_jobs_specifics as s ON b.job_id = s.job_id"

nyc_jobs_combination <- dbGetQuery(my_connection, nyc_jobs_combination_sql)

dbDisconnect(my_connection)

nyc_jobs_combination[,4:6] <- lapply(nyc_jobs_combination[,4:6], tolower)

nrow(nyc_jobs_combination)
```

Showing head of table
```{r}
head(nyc_jobs_combination)
```

#### Approach

For each job posting (row) we want to find all mentions of data science skills, scraping them and compiling them in new columns. We'll use regex to search the strings. The transformed dataframe will be:
- business_title | job_description | minimum_qual | preferred_skills | scraped_skills_title | scraped_skills_description | scraped_skills_qual | scraped_skills_skills

The final dataframe will remove duplicates from the "scraped" columns, compile all scraped skills into a list.

First we need to define the skills we'll search for. These keywords were generated using a combination of domain knowledge, Google Ads keyword planner, and skimming the jobs data set for requested skills. While leveraging regular expressions could make this list of keywords shorter, because there are generally so many keywords of interest, the amount of time to write regex for each would be burdensome. Therefore for this project we will write whole keywords, while avoiding ambiguity.

```{r}
data_acq_keys <- tolower(
  c("SQL", "MySQL", "NoSQL", "data acquisition", "database", "dbms", "JSON", "API", "data entry", "data scraping", "digitalocean", "mongodb", "postgre", "mariadb", "signal reception", "data extraction", "web scraping", "etl", "data collection", "web crawling", "data warehouse", "azure", "Amazon Web Services" )
)
data_prep_keys <- tolower(
  c("Alteryx", "Knime",  "data engineering", "preprocessing", "data preparation", "data cleaning", "data wrangling", "data transformation", "data integration", "normalization", "imputation", "data formatting", "data merging", "data enrichment", "data augmentation", "data sampling","data reduction", "dplyr", "stringr", "pandas" )
)
data_exp_keys <- tolower(
  c("Tableau", "feature selection", "data engineering", "data exploration", "EDA", "Descriptive Statistics", "Data Profiling", "Data Quality Assessment", "Data Sampling", "Dimensionality Reduction", "dimension reduction", "Feature Engineering", "Correlation Analysis", "Outlier Detection", "Pattern Identification", "databricks","data quality analysis","data summary", "data explorer", "exploratory data analysis", "skimr", "kusto", "azure data explorer", "data mining")
)
data_anal_keys <- tolower(
  c( "data modeling", "machine learning", "data analysis", "artificial intelligence", "qualitative analysis", "predictive analysis", "regression", "Statistical Analysis", "statistical analytics", "Cluster Analysis", "Clustering Analysis", "Hypothesis Testing", "A/B Testing", "Data Mining", "deep learning", "natural language processing", "ensemble methods", "scikit-learn", "pytorch", "spacy", "data analytics","pandas", "RStudio", "spss" )
)
data_vis_keys <- tolower(
  c("Tableau", "PowerBI", "quantitative findings", "creating graphs", "chart creation", "data reporting", "data visualization", "Data Storytelling", "Data Presentation", "Information Design", "Dashboard Creation", "Infographics", "network visualization", "user experience design", "datawrapper", "apache superset")
)

data_science_categories <- list(
  data_acquisition = data_acq_keys, 
  data_preparation = data_prep_keys,
  data_exploration = data_exp_keys, 
  data_analysis = data_anal_keys, 
  data_visualization = data_vis_keys
)
```

```{r}
nyc_jobs_combination_sql <- "
SELECT b.job_id, b.business_title, b.job_category, s.job_description, s.minimum_qual, s.preferred_skills
FROM nyc_jobs_basics as b 
JOIN nyc_jobs_specifics as s ON b.job_id = s.job_id"

nyc_jobs_combination <- dbGetQuery(my_connection, nyc_jobs_combination_sql)

dbDisconnect(my_connection)
nyc_jobs_combination[,4:6] <- lapply(nyc_jobs_combination[,4:6], tolower)
head(nyc_jobs_combination)
```

```{r keyword mutation}
# Create the new columns in the data frame which count up keyword matches within each category
for (category_name in names(data_science_categories)) {
  nyc_jobs_combination[[category_name]] <- as_tibble(sapply(data_science_categories[[category_name]],grepl,apply(nyc_jobs_combination[, 4:6], 1, paste, collapse = " "))) %>%
    mutate(!!category_name := as.integer(rowSums(across(where(is.logical)))), .keep="none") %>%
    pull(!!category_name)
}
# Create a column for total data science matches in the data frame
nyc_jobs_combination <- nyc_jobs_combination |>
  mutate(total_datascience_keywords = as.integer(rowSums(across(data_acquisition:data_visualization))))
glimpse(nyc_jobs_combination)
```
Using the above dataframe's keyword counts, we can determine which parts of the data science workflow are most in demand. (Calculated in R chunk below)
- Data Analysis keywords accounted for 26.7% of all data science keywords post in current NYC job postings
- Data Acquisition keywords accounted for 25.4% 
- Data Exploration keywords accounted for 24.4%
- Data Preparation keywords accounted for 21.2%
- Data Visualization keywords accounted for 2.3%

```{r}
round(sum(nyc_jobs_combination$data_acquisition)/sum(nyc_jobs_combination$total_datascience_keywords),3)
round(sum(nyc_jobs_combination$data_preparation)/sum(nyc_jobs_combination$total_datascience_keywords),3)
round(sum(nyc_jobs_combination$data_exploration)/sum(nyc_jobs_combination$total_datascience_keywords),3)
round(sum(nyc_jobs_combination$data_analysis)/sum(nyc_jobs_combination$total_datascience_keywords),3)
round(sum(nyc_jobs_combination$data_visualization)/sum(nyc_jobs_combination$total_datascience_keywords),3)
```

Now that we have an understanding of the most in demand data science skill categories, next we'll return a list of those skills themselves for each job posting. By quantifying the count of each skill, we can answer the question "Which data science skills are most in demand"?

Combining data science keyword category lists 
```{r}
ds_keywords_full <- c(data_acq_keys,data_prep_keys,data_exp_keys,data_anal_keys,data_vis_keys)
```

Extracting keywords from rows
```{r}
#Use str_extract instead of str_extract_all to avoid duplicates
#Can look only in job_description and preferred_skills for simplicity; should encompass all keywords?

#will work on this in the morning
library(stringr)
str_extract_all(ds_keywords_full, str_c(a, collapse="|"))[[1]]
unlist(regmatches(b, gregexpr(paste0(a, collapse = "|")

```

